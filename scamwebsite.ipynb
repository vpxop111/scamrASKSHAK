{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5iVMgKITfZULZAJD7qe+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpxop111/scamrASKSHAK/blob/master/scamwebsite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIIyv3dj5hJx",
        "outputId": "c3a7ad10-9f3a-480d-d140-1c6c4c66a546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j57WuOeq_Qjf",
        "outputId": "b7c98316-214d-45d8-fd67-6ab86a0d5e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/verified_online.csv'\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "\n",
        "\n",
        "# Ensure the DataFrame contains the columns 'url' and 'label'\n",
        "assert 'url' in df.columns and 'label' in df.columns, \"CSV file must contain 'url' and 'label' columns.\"\n",
        "\n",
        "# Function to preprocess URLs: Tokenization and padding\n",
        "def preprocess_url(url):\n",
        "    url = url.lower()\n",
        "    url = re.sub(r'https?://', '', url)  # Remove protocol\n",
        "    url = re.sub(r'www\\.', '', url)       # Remove 'www'\n",
        "    return ' '.join(list(url))            # Tokenize by characters\n",
        "\n",
        "class URLDataset(Dataset):\n",
        "    def __init__(self, urls, labels, max_length):\n",
        "        self.urls = urls\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = LabelEncoder()\n",
        "        self.tokenizer.fit([char for url in urls for char in preprocess_url(url)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urls)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        url = preprocess_url(self.urls[idx])\n",
        "        url_tokens = self.tokenizer.transform(list(url))\n",
        "        url_padded = np.pad(url_tokens, (0, self.max_length - len(url_tokens)), mode='constant')\n",
        "        return torch.tensor(url_padded, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Parameters\n",
        "max_length = 50\n",
        "batch_size = 2\n",
        "num_classes = 2\n",
        "embedding_dim = 10\n",
        "hidden_dim = 20\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Data preparation\n",
        "X = df['url'].values\n",
        "y = df['label'].values\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = URLDataset(X_train, y_train, max_length)\n",
        "val_dataset = URLDataset(X_val, y_val, max_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        h0 = torch.zeros(self.rnn.num_layers, x.size(0), hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "vocab_size = len(train_dataset.tokenizer.classes_)\n",
        "model = RNNModel(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for urls, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(urls)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for urls, labels in val_loader:\n",
        "            outputs = model(urls)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
        "evaluate_model(model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "bfHUdIYq_afY",
        "outputId": "6138fff2-4976-4f82-fef1-d4fbd65171f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "CSV file must contain 'url' and 'label' columns.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cbb9945db4ec>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Ensure the DataFrame contains the columns 'url' and 'label'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'url'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'label'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CSV file must contain 'url' and 'label' columns.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Function to preprocess URLs: Tokenization and padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: CSV file must contain 'url' and 'label' columns."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datallm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpPSrljWK1MW",
        "outputId": "7571c5ae-d7f2-49f4-cb8d-82bfc74cc85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datallm\n",
            "  Downloading datallm-0.2.6-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httpx>=0.25.1 (from datallm)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from datallm) (2.1.4)\n",
            "Requirement already satisfied: pyarrow>=14.0.2 in /usr/local/lib/python3.10/dist-packages (from datallm) (14.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from datallm) (2.8.2)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from datallm) (13.7.1)\n",
            "Collecting tenacity<9.0.0,>=8.2.3 (from datallm)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->datallm) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->datallm) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.1->datallm)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->datallm) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->datallm) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.1->datallm)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.5.3->datallm) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.5.3->datallm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.5.3->datallm) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.5.3->datallm) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->datallm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->datallm) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->datallm) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->datallm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->datallm) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->datallm) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.5.3->datallm) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.1->datallm) (1.2.2)\n",
            "Downloading datallm-0.2.6-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, h11, httpcore, httpx, datallm\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed datallm-0.2.6 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 tenacity-8.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming you're running this in Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/dataset_phishing.csv'\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "\n",
        "# Load and preprocess data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Features and target\n",
        "features = [\n",
        "    'url', 'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn',\n",
        "    'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
        "    'ratio_digits_host', 'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain', 'nb_subdomains',\n",
        "    'prefix_suffix', 'random_domain', 'shortening_service', 'path_extension', 'nb_redirection', 'nb_external_redirection',\n",
        "    'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path',\n",
        "    'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path',\n",
        "    'phish_hints', 'domain_in_brand', 'brand_in_subdomain', 'brand_in_path', 'suspecious_tld',\n",
        "    'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS',\n",
        "    'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors', 'login_form',\n",
        "    'external_favicon', 'links_in_tags', 'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe',\n",
        "    'popup_window', 'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title',\n",
        "    'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length', 'domain_age'\n",
        "]\n",
        "\n",
        "df['status'] = df['status'].map({'phishing': 1, 'legitimate': 0})\n",
        "\n",
        "X = df[features]\n",
        "y = df['status']\n",
        "\n",
        "# Convert 'url' column to numerical\n",
        "X['url'] = pd.factorize(X['url'])[0]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define the RNN model\n",
        "class PhishingRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
        "        super(PhishingRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_tensor.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 2\n",
        "num_layers = 1\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = PhishingRNN(input_size, hidden_size, output_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train_tensor.unsqueeze(1))\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluating the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train_tensor.unsqueeze(1)).argmax(dim=1)\n",
        "    y_pred_test = model(X_test_tensor.unsqueeze(1)).argmax(dim=1)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train.numpy())\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test.numpy())\n",
        "\n",
        "    print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test.numpy()))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test.numpy()))\n",
        "\n",
        "# Function to test a new URL\n",
        "def test_new_url(url_features):\n",
        "    assert len(url_features) == len(features), f\"Expected {len(features)} features, but got {len(url_features)}.\"\n",
        "\n",
        "    # Convert url to numerical value\n",
        "    url_features[0] = pd.factorize([url_features[0]])[0][0]\n",
        "\n",
        "    model.eval()\n",
        "    url_features_scaled = scaler.transform([url_features])\n",
        "    url_tensor = torch.tensor(url_features_scaled, dtype=torch.float32).unsqueeze(1)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(url_tensor).argmax(dim=1).item()\n",
        "    return 'Phishing' if prediction == 1 else 'Legitimate'\n",
        "\n",
        "# Example usage\n",
        "new_url_features = [\n",
        "    'https://www.youtube.com', 23, 15, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, 443, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 0, 3, 7, 3, 7, 7, 7, 3.29, 7.0, 3.0, 0, 0, 0, 0, 0, 10, 1.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0, 0, 1.0, 0.0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 365, 5844\n",
        "]\n",
        "\n",
        "# Test the new URL\n",
        "result = test_new_url(new_url_features)\n",
        "print(f'The new URL is classified as: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9_fHF01K3-b",
        "outputId": "27f3872a-249f-4c53-ea21-e9d24b9fe01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-cad56c18bccb>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['url'] = pd.factorize(X['url'])[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.2955\n",
            "Epoch [20/100], Loss: 0.2404\n",
            "Epoch [30/100], Loss: 0.2186\n",
            "Epoch [40/100], Loss: 0.2021\n",
            "Epoch [50/100], Loss: 0.1836\n",
            "Epoch [60/100], Loss: 0.1635\n",
            "Epoch [70/100], Loss: 0.1419\n",
            "Epoch [80/100], Loss: 0.1198\n",
            "Epoch [90/100], Loss: 0.0991\n",
            "Epoch [100/100], Loss: 0.0838\n",
            "Train Accuracy: 0.9718\n",
            "Test Accuracy: 0.9405\n",
            "Confusion Matrix:\n",
            " [[1347   75]\n",
            " [  95 1341]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94      1422\n",
            "           1       0.95      0.93      0.94      1436\n",
            "\n",
            "    accuracy                           0.94      2858\n",
            "   macro avg       0.94      0.94      0.94      2858\n",
            "weighted avg       0.94      0.94      0.94      2858\n",
            "\n",
            "The new URL is classified as: Phishing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-cad56c18bccb>:121: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
            "  url_features[0] = pd.factorize([url_features[0]])[0][0]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hr5HFMiD7vVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade python-whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "qSw9WwhiuPd9",
        "outputId": "57488c7c-722b-469a-924a-196cb3ed1e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Downloading python_whois-0.9.4-py3-none-any.whl (103 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m102.4/103.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-whois\n",
            "Successfully installed python-whois-0.9.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whois"
                ]
              },
              "id": "f560c217efeb4dbc9ffc28324191540b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import whois\n",
        "import tldextract\n",
        "import datetime\n",
        "import socket\n",
        "\n",
        "def is_valid_ip(ip):\n",
        "    \"\"\"Check if the hostname is an IP address\"\"\"\n",
        "    try:\n",
        "        socket.inet_aton(ip)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Basic URL parsing\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Having IP Address\n",
        "    features['having_IP_Address'] = -1 if is_valid_ip(parsed_url.hostname) else 1\n",
        "\n",
        "    # URL Length\n",
        "    url_len = len(url)\n",
        "    features['URL_Length'] = 1 if url_len < 54 else (0 if 54 <= url_len <= 75 else -1)\n",
        "\n",
        "    # Shortening Service\n",
        "    shortening_services = ['bit.ly', 'tinyurl.com', 'goo.gl']\n",
        "    features['Shortining_Service'] = -1 if any(service in url for service in shortening_services) else 1\n",
        "\n",
        "    # Having '@' Symbol\n",
        "    features['having_At_Symbol'] = -1 if '@' in url else 1\n",
        "\n",
        "    # Double Slash Redirecting\n",
        "    features['double_slash_redirecting'] = -1 if url.count('//') > 1 else 1\n",
        "\n",
        "    # Prefix-Suffix in Domain\n",
        "    features['Prefix_Suffix'] = -1 if '-' in parsed_url.netloc else 1\n",
        "\n",
        "    # Having Sub Domain\n",
        "    subdomains = domain_info.subdomain.split('.')\n",
        "    if len(subdomains) == 0:\n",
        "        features['having_Sub_Domain'] = -1\n",
        "    elif len(subdomains) == 1:\n",
        "        features['having_Sub_Domain'] = 0\n",
        "    else:\n",
        "        features['having_Sub_Domain'] = 1\n",
        "\n",
        "    # SSLfinal State (simplified check)\n",
        "    features['SSLfinal_State'] = 1 if parsed_url.scheme == 'https' else -1\n",
        "\n",
        "    # Domain Registration Length (simplified for example)\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        if whois_info.expiration_date and whois_info.creation_date:\n",
        "            if isinstance(whois_info.expiration_date, list):\n",
        "                expiration_date = whois_info.expiration_date[0]\n",
        "            else:\n",
        "                expiration_date = whois_info.expiration_date\n",
        "            registration_length = (expiration_date - whois_info.creation_date).days\n",
        "            features['Domain_registeration_length'] = 1 if registration_length > 365 else -1\n",
        "        else:\n",
        "            features['Domain_registeration_length'] = -1\n",
        "    except:\n",
        "        features['Domain_registeration_length'] = -1\n",
        "\n",
        "    # Favicon (simplified for example)\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        favicon_tag = soup.find('link', rel='shortcut icon')\n",
        "        features['Favicon'] = -1 if favicon_tag and parsed_url.netloc not in favicon_tag.get('href', '') else 1\n",
        "    except:\n",
        "        features['Favicon'] = -1\n",
        "\n",
        "    # Port (simplified check)\n",
        "    features['port'] = -1 if parsed_url.port and parsed_url.port != 80 and parsed_url.port != 443 else 1\n",
        "\n",
        "    # HTTPS Token in URL\n",
        "    features['HTTPS_token'] = -1 if 'https-' in parsed_url.hostname else 1\n",
        "\n",
        "    # Request URL (simplified for example)\n",
        "    try:\n",
        "        external_resources = [tag.get('src') for tag in soup.find_all('script', src=True)] + \\\n",
        "                             [tag.get('href') for tag in soup.find_all('link', href=True)]\n",
        "        features['Request_URL'] = 1 if all(parsed_url.netloc in resource for resource in external_resources) else -1\n",
        "    except:\n",
        "        features['Request_URL'] = -1\n",
        "\n",
        "    # URL of Anchor\n",
        "    try:\n",
        "        links = soup.find_all('a', href=True)\n",
        "        total_links = len(links)\n",
        "        if total_links > 0:\n",
        "            external_links = sum([1 for link in links if parsed_url.netloc not in link['href']])\n",
        "            features['URL_of_Anchor'] = -1 if external_links > (total_links * 0.67) else (0 if (total_links * 0.31) <= external_links <= (total_links * 0.67) else 1)\n",
        "        else:\n",
        "            features['URL_of_Anchor'] = 1\n",
        "    except:\n",
        "        features['URL_of_Anchor'] = 1\n",
        "\n",
        "    # Links in Tags (script, link)\n",
        "    try:\n",
        "        script_links = len(soup.find_all('script', src=True))\n",
        "        link_links = len(soup.find_all('link', href=True))\n",
        "        total_links = script_links + link_links\n",
        "        features['Links_in_tags'] = 1 if total_links == 0 else (0 if total_links < 2 else -1)\n",
        "    except:\n",
        "        features['Links_in_tags'] = -1\n",
        "\n",
        "    # Server Form Handler (SFH)\n",
        "    try:\n",
        "        forms = soup.find_all('form', action=True)\n",
        "        if len(forms) > 0:\n",
        "            action_values = [form['action'] for form in forms]\n",
        "            if any('mailto:' in action for action in action_values):\n",
        "                features['SFH'] = -1\n",
        "            elif any('https' in action or 'http' in action for action in action_values):\n",
        "                features['SFH'] = 1\n",
        "            else:\n",
        "                features['SFH'] = 0\n",
        "        else:\n",
        "            features['SFH'] = 1\n",
        "    except:\n",
        "        features['SFH'] = -1\n",
        "\n",
        "    # Submitting to Email\n",
        "    features['Submitting_to_email'] = -1 if any('mailto:' in form['action'] for form in soup.find_all('form', action=True)) else 1\n",
        "\n",
        "    # Abnormal URL\n",
        "    features['Abnormal_URL'] = 1 if domain_info.domain in parsed_url.hostname else -1\n",
        "\n",
        "    # Redirect (simplified for example)\n",
        "    try:\n",
        "        features['Redirect'] = 1 if len(response.history) <= 1 else 0\n",
        "    except:\n",
        "        features['Redirect'] = 0\n",
        "\n",
        "    # On Mouseover\n",
        "    features['on_mouseover'] = -1 if bool(soup.find_all(attrs={\"onmouseover\": True})) else 1\n",
        "\n",
        "    # RightClick\n",
        "    features['RightClick'] = -1 if bool(soup.find_all(attrs={\"oncontextmenu\": True})) else 1\n",
        "\n",
        "    # popUpWindow (simplified for example)\n",
        "    features['popUpWidnow'] = -1 if bool(soup.find('script', text=re.compile('window.open'))) else 1\n",
        "\n",
        "    # Iframe\n",
        "    features['Iframe'] = -1 if bool(soup.find('iframe')) else 1\n",
        "\n",
        "    # Age of Domain\n",
        "    try:\n",
        "        features['age_of_domain'] = 1 if (datetime.datetime.now() - whois_info.creation_date).days > 180 else -1\n",
        "    except:\n",
        "        features['age_of_domain'] = -1\n",
        "\n",
        "    # DNS Record\n",
        "    try:\n",
        "        features['DNSRecord'] = 1 if bool(whois_info.domain_name) else -1\n",
        "    except:\n",
        "        features['DNSRecord'] = -1\n",
        "\n",
        "    # Web Traffic (placeholder, requires external data)\n",
        "    features['web_traffic'] = 0  # Placeholder\n",
        "\n",
        "    # PageRank (placeholder, requires external data)\n",
        "    features['Page_Rank'] = 0  # Placeholder\n",
        "\n",
        "    # Google Index (simplified)\n",
        "    try:\n",
        "        response = requests.get(f'https://www.google.com/search?q=site:{parsed_url.hostname}')\n",
        "        features['Google_Index'] = 1 if 'did not match any documents' not in response.text else -1\n",
        "    except:\n",
        "        features['Google_Index'] = -1\n",
        "\n",
        "    # Links Pointing to Page (simplified)\n",
        "    try:\n",
        "        features['Links_pointing_to_page'] = -1 if len(links) == 0 else (0 if len(links) <= 2 else 1)\n",
        "    except:\n",
        "        features['Links_pointing_to_page'] = 0\n",
        "\n",
        "    # Statistical Report (placeholder)\n",
        "    features['Statistical_report'] = 0  # Placeholder\n",
        "\n",
        "    # Print all features\n",
        "    for key, value in features.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example usage\n",
        "url = \"http://youtube.com\"\n",
        "extract_features(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS9pMK0Ut_Dr",
        "outputId": "277179ea-7c9f-4346-fcd5-8d7a47f402bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-384d3981e745>:152: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  features['popUpWidnow'] = -1 if bool(soup.find('script', text=re.compile('window.open'))) else 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "having_IP_Address: 1\n",
            "URL_Length: 1\n",
            "Shortining_Service: 1\n",
            "having_At_Symbol: 1\n",
            "double_slash_redirecting: 1\n",
            "Prefix_Suffix: 1\n",
            "having_Sub_Domain: 0\n",
            "SSLfinal_State: -1\n",
            "Domain_registeration_length: -1\n",
            "Favicon: 1\n",
            "port: 1\n",
            "HTTPS_token: 1\n",
            "Request_URL: -1\n",
            "URL_of_Anchor: 0\n",
            "Links_in_tags: -1\n",
            "SFH: 1\n",
            "Submitting_to_email: 1\n",
            "Abnormal_URL: 1\n",
            "Redirect: 0\n",
            "on_mouseover: 1\n",
            "RightClick: 1\n",
            "popUpWidnow: 1\n",
            "Iframe: -1\n",
            "age_of_domain: -1\n",
            "DNSRecord: 1\n",
            "web_traffic: 0\n",
            "Page_Rank: 0\n",
            "Google_Index: 1\n",
            "Links_pointing_to_page: 1\n",
            "Statistical_report: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'having_IP_Address': 1,\n",
              " 'URL_Length': 1,\n",
              " 'Shortining_Service': 1,\n",
              " 'having_At_Symbol': 1,\n",
              " 'double_slash_redirecting': 1,\n",
              " 'Prefix_Suffix': 1,\n",
              " 'having_Sub_Domain': 0,\n",
              " 'SSLfinal_State': -1,\n",
              " 'Domain_registeration_length': -1,\n",
              " 'Favicon': 1,\n",
              " 'port': 1,\n",
              " 'HTTPS_token': 1,\n",
              " 'Request_URL': -1,\n",
              " 'URL_of_Anchor': 0,\n",
              " 'Links_in_tags': -1,\n",
              " 'SFH': 1,\n",
              " 'Submitting_to_email': 1,\n",
              " 'Abnormal_URL': 1,\n",
              " 'Redirect': 0,\n",
              " 'on_mouseover': 1,\n",
              " 'RightClick': 1,\n",
              " 'popUpWidnow': 1,\n",
              " 'Iframe': -1,\n",
              " 'age_of_domain': -1,\n",
              " 'DNSRecord': 1,\n",
              " 'web_traffic': 0,\n",
              " 'Page_Rank': 0,\n",
              " 'Google_Index': 1,\n",
              " 'Links_pointing_to_page': 1,\n",
              " 'Statistical_report': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset_phishing.csv')\n",
        "numerical_cols = df.select_dtypes(include=['float', 'int']).columns\n",
        "X = df[numerical_cols].values\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Normalize the data (if necessary)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Define the model\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1  # Binary classification\n",
        "\n",
        "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Since it's a binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs.squeeze(), y_batch)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# Evaluation on test data\n",
        "model.eval()\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        predicted = torch.round(torch.sigmoid(outputs.squeeze()))\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert predictions to numpy array\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test.numpy(), y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat = confusion_matrix(y_test.numpy(), y_pred)\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "UkgXvIYkflK6",
        "outputId": "4b13c3dd-3856-40b1-b384-fe285f0ce2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'http://www.crestonwood.com/router.php'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-2b18fb963613>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Normalize the data (if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Convert to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \"\"\"\n\u001b[1;32m    874\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    876\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'http://www.crestonwood.com/router.php'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NakG9VmAgzNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy requests beautifulsoup4 python-whois tldextract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "q6PvODITbVw-",
        "outputId": "ffe52c49-2e54-432e-beb0-2e1ca3b43e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.15.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Downloading python_whois-0.9.4-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-whois\n",
            "Successfully installed python-whois-0.9.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whois"
                ]
              },
              "id": "71942715c8b24821a1c7fdf32888e2ea"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import whois\n",
        "import tldextract\n",
        "import datetime\n",
        "\n",
        "# Mount your Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load and preprocess data\n",
        "data_path = '/content/drive/MyDrive/dataset_phishing.csv'\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Basic URL Length Features\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.hostname) if parsed_url.hostname else 0\n",
        "\n",
        "    # Token Counts\n",
        "    features['nb_dots'] = url.count('.')\n",
        "    features['nb_hyphens'] = url.count('-')\n",
        "    features['nb_at'] = url.count('@')\n",
        "    features['nb_qm'] = url.count('?')\n",
        "    features['nb_and'] = url.count('&')\n",
        "    features['nb_or'] = url.count('|')\n",
        "    features['nb_eq'] = url.count('=')\n",
        "    features['nb_underscore'] = url.count('_')\n",
        "    features['nb_tilde'] = url.count('~')\n",
        "    features['nb_percent'] = url.count('%')\n",
        "    features['nb_slash'] = url.count('/')\n",
        "    features['nb_star'] = url.count('*')\n",
        "    features['nb_colon'] = url.count(':')\n",
        "    features['nb_comma'] = url.count(',')\n",
        "    features['nb_semicolumn'] = url.count(';')\n",
        "    features['nb_dollar'] = url.count('$')\n",
        "    features['nb_space'] = url.count(' ')\n",
        "    features['nb_www'] = url.count('www')\n",
        "    features['nb_com'] = url.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    # Path Features\n",
        "    features['http_in_path'] = 'http' in parsed_url.path\n",
        "    features['https_token'] = 'https' in url\n",
        "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0\n",
        "\n",
        "    # Domain Features\n",
        "    features['punycode'] = parsed_url.hostname.encode('idna').decode('utf-8') != parsed_url.hostname\n",
        "    features['shortening_service'] = any(service in url for service in ['bit.ly', 'tinyurl.com', 'goo.gl'])\n",
        "    features['port'] = parsed_url.port if parsed_url.port else -1\n",
        "    features['tld_in_path'] = any(tld in parsed_url.path for tld in ['.com', '.net', '.org'])\n",
        "    features['tld_in_subdomain'] = domain_info.suffix in parsed_url.hostname\n",
        "    features['path_extension'] = any(ext in parsed_url.path for ext in ['.php', '.asp', '.aspx', '.jsp'])\n",
        "    features['prefix_suffix'] = any(part in url for part in ['.htm', '.html'])\n",
        "    features['random_domain'] = any(sub in parsed_url.hostname for sub in ['random', '1234', 'xyz'])\n",
        "\n",
        "    # Fetch Page Content for More Features\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Count Links in the Page\n",
        "        links = soup.find_all('a')\n",
        "        features['nb_hyperlinks'] = len(links)\n",
        "        features['ratio_extHyperlinks'] = len([link for link in links if link.get('href', '').startswith('http')]) / len(links) if len(links) > 0 else 0\n",
        "        features['ratio_intHyperlinks'] = len([link for link in links if link.get('href', '') and not link.get('href', '').startswith('http')]) / len(links) if len(links) > 0 else 0\n",
        "        features['ratio_nullHyperlinks'] = len([link for link in links if not link.get('href')]) / len(links) if len(links) > 0 else 0\n",
        "\n",
        "        # Check for Login Form\n",
        "        features['login_form'] = int(bool(soup.find('form') and (soup.find('input', {'type': 'password'}) or soup.find('input', {'type': 'text'}))))\n",
        "\n",
        "        # Check for External CSS\n",
        "        css_links = soup.find_all('link', {'rel': 'stylesheet'})\n",
        "        features['nb_extCSS'] = len(css_links)\n",
        "\n",
        "        # External Links and Media\n",
        "        media_links = soup.find_all(['img', 'video', 'audio'])\n",
        "        features['ratio_extMedia'] = len(media_links) / len(links) if len(links) > 0 else 0\n",
        "\n",
        "        # Check for Popups and Iframes\n",
        "        features['iframe'] = int(bool(soup.find('iframe')))\n",
        "        features['popup_window'] = int(bool(soup.find('script', text=re.compile('window.open'))))\n",
        "\n",
        "        # Additional Features\n",
        "        features['sfh'] = int(bool(soup.find('form', action=lambda x: x and x.startswith('http'))))\n",
        "        features['safe_anchor'] = int(all(link.get('rel') != 'nofollow' for link in links))\n",
        "        features['onmouseover'] = int(bool(soup.find_all(attrs={\"onmouseover\": True})))\n",
        "        features['right_clic'] = int(bool(soup.find_all(attrs={\"oncontextmenu\": True})))\n",
        "        features['empty_title'] = int(bool(soup.find('title') and not soup.find('title').text.strip()))\n",
        "        features['domain_in_title'] = int(domain_info.domain in (soup.find('title').text if soup.find('title') else ''))\n",
        "        features['domain_with_copyright'] = int('copyright' in (soup.find('body').text if soup.find('body') else '').lower())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        features['nb_hyperlinks'] = 0\n",
        "        features['ratio_extHyperlinks'] = 0\n",
        "        features['ratio_intHyperlinks'] = 0\n",
        "        features['ratio_nullHyperlinks'] = 0\n",
        "        features['nb_extCSS'] = 0\n",
        "        features['ratio_extMedia'] = 0\n",
        "        features['iframe'] = 0\n",
        "        features['popup_window'] = 0\n",
        "        features['sfh'] = 0\n",
        "        features['safe_anchor'] = 0\n",
        "        features['onmouseover'] = 0\n",
        "        features['right_clic'] = 0\n",
        "        features['empty_title'] = 0\n",
        "        features['domain_in_title'] = 0\n",
        "        features['domain_with_copyright'] = 0\n",
        "\n",
        "    # WHOIS Registration Features\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        features['whois_registered_domain'] = int(bool(whois_info.domain_name))\n",
        "        features['domain_registration_length'] = whois_info.domain_age if whois_info.domain_age else -1\n",
        "        features['domain_age'] = (datetime.datetime.now() - whois_info.creation_date).days if whois_info.creation_date else -1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching WHOIS info: {e}\")\n",
        "        features['whois_registered_domain'] = 0\n",
        "        features['domain_registration_length'] = -1\n",
        "        features['domain_age'] = -1\n",
        "\n",
        "    return features\n",
        "\n",
        "# Map the 'status' column to numeric values before splitting the data\n",
        "df['status'] = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
        "\n",
        "# Feature and target selection\n",
        "features = [\n",
        "    'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn',\n",
        "    'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
        "    'ratio_digits_host', 'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain', 'nb_subdomains',\n",
        "    'prefix_suffix', 'random_domain', 'shortening_service', 'path_extension', 'nb_redirection', 'nb_external_redirection',\n",
        "    'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path',\n",
        "    'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path',\n",
        "    'phish_hints', 'domain_in_brand', 'brand_in_subdomain', 'brand_in_path', 'suspecious_tld', 'statistical_report',\n",
        "    'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS',\n",
        "    'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors', 'login_form',\n",
        "    'external_favicon', 'links_in_tags', 'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe',\n",
        "    'popup_window', 'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title',\n",
        "    'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length', 'domain_age'\n",
        "]\n",
        "\n",
        "X = df[features].values\n",
        "y = df['status'].values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train.unsqueeze(1))\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train.unsqueeze(1)).round()\n",
        "    y_pred_test = model(X_test.unsqueeze(1)).round()\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Predicting new URL\n",
        "new_url = 'https://www.youtube.com'\n",
        "new_features = extract_features(new_url)\n",
        "new_features = pd.DataFrame([new_features])\n",
        "new_features = scaler.transform(new_features)\n",
        "new_features_tensor = torch.tensor(new_features, dtype=torch.float32).unsqueeze(1)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    new_prediction = model(new_features_tensor).round()\n",
        "    print(f'Prediction for {new_url}: {\"Phishing\" if new_prediction.item() == 1 else \"Legitimate\"}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "1btKXJOEbeJe",
        "outputId": "6fe499a2-36bc-4bed-aa12-8d44156a42b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.4442\n",
            "Epoch [20/100], Loss: 0.3165\n",
            "Epoch [30/100], Loss: 0.2697\n",
            "Epoch [40/100], Loss: 0.2510\n",
            "Epoch [50/100], Loss: 0.2397\n",
            "Epoch [60/100], Loss: 0.2323\n",
            "Epoch [70/100], Loss: 0.2270\n",
            "Epoch [80/100], Loss: 0.2222\n",
            "Epoch [90/100], Loss: 0.2173\n",
            "Epoch [100/100], Loss: 0.2118\n",
            "Training Accuracy: 0.9170\n",
            "Test Accuracy: 0.9147\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.92      0.92      1157\n",
            "         1.0       0.92      0.90      0.91      1129\n",
            "\n",
            "    accuracy                           0.91      2286\n",
            "   macro avg       0.91      0.91      0.91      2286\n",
            "weighted avg       0.91      0.91      0.91      2286\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1070   87]\n",
            " [ 108 1021]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-1c737c92aa6b>:97: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  features['popup_window'] = int(bool(soup.find('script', text=re.compile('window.open'))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching WHOIS info: unsupported operand type(s) for -: 'datetime.datetime' and 'list'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 52 features, but StandardScaler is expecting 83 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1c737c92aa6b>\u001b[0m in \u001b[0;36m<cell line: 242>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0mnew_features_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 52 features, but StandardScaler is expecting 83 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "import tldextract\n",
        "import whois\n",
        "import datetime\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Basic URL Length Features\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.hostname) if parsed_url.hostname else 0\n",
        "\n",
        "    # Token Counts\n",
        "    features['nb_dots'] = url.count('.')\n",
        "    features['nb_hyphens'] = url.count('-')\n",
        "    features['nb_at'] = url.count('@')\n",
        "    features['nb_qm'] = url.count('?')\n",
        "    features['nb_and'] = url.count('&')\n",
        "    features['nb_or'] = url.count('|')\n",
        "    features['nb_eq'] = url.count('=')\n",
        "    features['nb_underscore'] = url.count('_')\n",
        "    features['nb_tilde'] = url.count('~')\n",
        "    features['nb_percent'] = url.count('%')\n",
        "    features['nb_slash'] = url.count('/')\n",
        "    features['nb_star'] = url.count('*')\n",
        "    features['nb_colon'] = url.count(':')\n",
        "    features['nb_comma'] = url.count(',')\n",
        "    features['nb_semicolumn'] = url.count(';')\n",
        "    features['nb_dollar'] = url.count('$')\n",
        "    features['nb_space'] = url.count(' ')\n",
        "    features['nb_www'] = url.count('www')\n",
        "    features['nb_com'] = url.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    # Path Features\n",
        "    features['http_in_path'] = 'http' in parsed_url.path\n",
        "    features['https_token'] = 'https' in url\n",
        "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0\n",
        "\n",
        "    # Domain Features\n",
        "    features['punycode'] = parsed_url.hostname.encode('idna').decode('utf-8') != parsed_url.hostname\n",
        "    features['shortening_service'] = any(service in url for service in ['bit.ly', 'tinyurl.com', 'goo.gl'])\n",
        "    features['port'] = parsed_url.port if parsed_url.port else -1\n",
        "    features['tld_in_path'] = any(tld in parsed_url.path for tld in ['.com', '.net', '.org'])\n",
        "    features['tld_in_subdomain'] = domain_info.suffix in parsed_url.hostname\n",
        "    features['path_extension'] = any(ext in parsed_url.path for ext in ['.php', '.asp', '.aspx', '.jsp'])\n",
        "    features['prefix_suffix'] = any(part in url for part in ['.htm', '.html'])\n",
        "    features['random_domain'] = any(sub in parsed_url.hostname for sub in ['random', '1234', 'xyz'])\n",
        "\n",
        "    # Fetch Page Content for More Features\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Count Links in the Page\n",
        "        links = soup.find_all('a')\n",
        "        features['nb_hyperlinks'] = len(links)\n",
        "        features['ratio_extHyperlinks'] = len([link for link in links if link.get('href', '').startswith('http')]) / len(links) if len(links) > 0 else 0\n",
        "        features['ratio_intHyperlinks'] = len([link for link in links if link.get('href', '') and not link.get('href', '').startswith('http')]) / len(links) if len(links) > 0 else 0\n",
        "        features['ratio_nullHyperlinks'] = len([link for link in links if not link.get('href')]) / len(links) if len(links) > 0 else 0\n",
        "\n",
        "        # Check for Login Form\n",
        "        features['login_form'] = int(bool(soup.find('form') and (soup.find('input', {'type': 'password'}) or soup.find('input', {'type': 'text'}))))\n",
        "\n",
        "        # Check for External CSS\n",
        "        css_links = soup.find_all('link', {'rel': 'stylesheet'})\n",
        "        features['nb_extCSS'] = len(css_links)\n",
        "\n",
        "        # External Links and Media\n",
        "        media_links = soup.find_all(['img', 'video', 'audio'])\n",
        "        features['ratio_extMedia'] = len(media_links) / len(links) if len(links) > 0 else 0\n",
        "\n",
        "        # Check for Popups and Iframes\n",
        "        features['iframe'] = int(bool(soup.find('iframe')))\n",
        "        features['popup_window'] = int(bool(soup.find('script', text=re.compile('window.open'))))\n",
        "\n",
        "        # Additional Features\n",
        "        features['sfh'] = int(bool(soup.find('form', action=lambda x: x and x.startswith('http'))))\n",
        "        features['safe_anchor'] = int(all(link.get('rel') != 'nofollow' for link in links))\n",
        "        features['onmouseover'] = int(bool(soup.find_all(attrs={\"onmouseover\": True})))\n",
        "        features['right_clic'] = int(bool(soup.find_all(attrs={\"oncontextmenu\": True})))\n",
        "        features['empty_title'] = int(bool(soup.find('title') and not soup.find('title').text.strip()))\n",
        "        features['domain_in_title'] = int(domain_info.domain in (soup.find('title').text if soup.find('title') else ''))\n",
        "        features['domain_with_copyright'] = int('copyright' in (soup.find('body').text if soup.find('body') else '').lower())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        features['nb_hyperlinks'] = 0\n",
        "        features['ratio_extHyperlinks'] = 0\n",
        "        features['ratio_intHyperlinks'] = 0\n",
        "        features['ratio_nullHyperlinks'] = 0\n",
        "        features['nb_extCSS'] = 0\n",
        "        features['ratio_extMedia'] = 0\n",
        "        features['iframe'] = 0\n",
        "        features['popup_window'] = 0\n",
        "        features['sfh'] = 0\n",
        "        features['safe_anchor'] = 0\n",
        "        features['onmouseover'] = 0\n",
        "        features['right_clic'] = 0\n",
        "        features['empty_title'] = 0\n",
        "        features['domain_in_title'] = 0\n",
        "        features['domain_with_copyright'] = 0\n",
        "\n",
        "    # WHOIS Registration Features\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        features['whois_registered_domain'] = int(bool(whois_info.domain_name))\n",
        "        features['domain_registration_length'] = whois_info.domain_age if whois_info.domain_age else -1\n",
        "        features['domain_age'] = (datetime.datetime.now() - whois_info.creation_date).days if whois_info.creation_date else -1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching WHOIS info: {e}\")\n",
        "        features['whois_registered_domain'] = 0\n",
        "        features['domain_registration_length'] = -1\n",
        "        features['domain_age'] = -1\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example usage\n",
        "url = 'https://www.example.com'\n",
        "extracted_features = extract_features(url)\n",
        "\n",
        "# Print features in list format\n",
        "feature_list = [f'{key}: {value}' for key, value in extracted_features.items()]\n",
        "for feature in feature_list:\n",
        "    print(feature)\n"
      ],
      "metadata": {
        "id": "Domw8wDy1iF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import whois\n",
        "import tldextract\n",
        "import datetime\n",
        "import socket\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "def is_valid_ip(ip):\n",
        "    \"\"\"Check if the hostname is an IP address\"\"\"\n",
        "    try:\n",
        "        socket.inet_aton(ip)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Basic URL parsing\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Basic URL Length Features\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.hostname) if parsed_url.hostname else 0\n",
        "\n",
        "    # Token Counts\n",
        "    tokens = ['.', '-', '@', '?', '&', '|', '=', '_', '~', '%', '/', '*', ':', ',', ';', '$', ' ']\n",
        "    token_names = ['nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore',\n",
        "                   'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma',\n",
        "                   'nb_semicolumn', 'nb_dollar', 'nb_space']\n",
        "\n",
        "    for token, name in zip(tokens, token_names):\n",
        "        features[name] = url.count(token)\n",
        "\n",
        "    features['nb_www'] = url.count('www')\n",
        "    features['nb_com'] = url.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    # Path Features\n",
        "    features['http_in_path'] = 'http' in parsed_url.path\n",
        "    features['https_token'] = 'https' in url\n",
        "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0\n",
        "\n",
        "    path_extensions = ['.php', '.asp', '.aspx', '.jsp']\n",
        "    features['path_extension'] = any(ext in parsed_url.path for ext in path_extensions)\n",
        "    features['prefix_suffix'] = any(part in url for part in ['.htm', '.html'])\n",
        "    random_strings = ['random', '1234', 'xyz']\n",
        "    features['random_domain'] = any(sub in parsed_url.hostname for sub in random_strings)\n",
        "\n",
        "    # Domain Features\n",
        "    features['punycode'] = parsed_url.hostname.encode('idna').decode('utf-8') != parsed_url.hostname\n",
        "    shortening_services = ['bit.ly', 'tinyurl.com', 'goo.gl']\n",
        "    features['shortening_service'] = any(service in url for service in shortening_services)\n",
        "    features['port'] = parsed_url.port if parsed_url.port else -1\n",
        "    tlds = ['.com', '.net', '.org']\n",
        "    features['tld_in_path'] = any(tld in parsed_url.path for tld in tlds)\n",
        "    features['tld_in_subdomain'] = domain_info.suffix in parsed_url.hostname\n",
        "\n",
        "    # Check if IP address is used as hostname\n",
        "    features['ip'] = int(is_valid_ip(parsed_url.hostname))\n",
        "\n",
        "    # Ratio of digits in hostname\n",
        "    features['ratio_digits_host'] = sum(c.isdigit() for c in parsed_url.hostname) / len(parsed_url.hostname) if len(parsed_url.hostname) > 0 else 0\n",
        "\n",
        "    # Abnormal subdomain\n",
        "    features['abnormal_subdomain'] = int(len(domain_info.subdomain.split('.')) > 2)\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['nb_subdomains'] = len(domain_info.subdomain.split('.')) if domain_info.subdomain else 0\n",
        "\n",
        "    # Initialize media ratios\n",
        "    features['ratio_intMedia'] = 0\n",
        "    features['ratio_extMedia'] = 0\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Count Links in the Page\n",
        "        links = soup.find_all('a')\n",
        "        total_links = len(links)\n",
        "        features['nb_hyperlinks'] = total_links\n",
        "\n",
        "        if total_links > 0:\n",
        "            ext_links = len([link for link in links if link.get('href', '').startswith('http')])\n",
        "            int_links = len([link for link in links if not link.get('href', '').startswith('http')])\n",
        "            null_links = len([link for link in links if not link.get('href')])\n",
        "\n",
        "            features['ratio_extHyperlinks'] = ext_links / total_links\n",
        "            features['ratio_intHyperlinks'] = int_links / total_links\n",
        "            features['ratio_nullHyperlinks'] = null_links / total_links\n",
        "        else:\n",
        "            features['ratio_extHyperlinks'] = 0\n",
        "            features['ratio_intHyperlinks'] = 0\n",
        "            features['ratio_nullHyperlinks'] = 0\n",
        "\n",
        "        # Calculate redirections\n",
        "        features['nb_redirection'] = len(response.history)\n",
        "        features['nb_external_redirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc != parsed_url.netloc])\n",
        "\n",
        "        # Internal and external redirection ratios\n",
        "        features['ratio_intRedirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc == parsed_url.netloc]) / len(response.history) if response.history else 0\n",
        "        features['ratio_extRedirection'] = features['nb_external_redirection'] / len(response.history) if response.history else 0\n",
        "\n",
        "        # Check for favicon\n",
        "        favicon_tag = soup.find('link', rel='shortcut icon')\n",
        "        features['external_favicon'] = int(favicon_tag and parsed_url.netloc not in favicon_tag.get('href', ''))\n",
        "\n",
        "        # Links in specific tags\n",
        "        script_links = len(soup.find_all('script', src=True))\n",
        "        link_links = len(soup.find_all('link', href=True))\n",
        "        features['links_in_tags'] = script_links + link_links\n",
        "\n",
        "        features['login_form'] = int(bool(soup.find('form') and\n",
        "                                            (soup.find('input', {'type': 'password'}) or\n",
        "                                             soup.find('input', {'type': 'text'}))))\n",
        "\n",
        "        # External CSS\n",
        "        css_links = soup.find_all('link', {'rel': 'stylesheet'})\n",
        "        features['nb_extCSS'] = len(css_links)\n",
        "\n",
        "        media_links = soup.find_all(['img', 'video', 'audio'])\n",
        "        features['ratio_extMedia'] = len(media_links) / len(links) if len(links) > 0 else 0\n",
        "        features['iframe'] = int(bool(soup.find('iframe')))\n",
        "        features['popup_window'] = int(bool(soup.find('script', text=re.compile('window.open'))))\n",
        "\n",
        "        safe_anchors = [link.get('rel') != 'nofollow' for link in links]\n",
        "        features['sfh'] = int(any(form.get('action', '').startswith('http') for form in soup.find_all('form')))\n",
        "        features['safe_anchor'] = int(all(safe_anchors))\n",
        "\n",
        "        features['onmouseover'] = int(bool(soup.find_all(attrs={\"onmouseover\": True})))\n",
        "        features['right_clic'] = int(bool(soup.find_all(attrs={\"oncontextmenu\": True})))\n",
        "        features['empty_title'] = int(bool(soup.find('title') and not soup.find('title').text.strip()))\n",
        "        features['domain_in_title'] = int(domain_info.domain in (soup.find('title').text if soup.find('title') else ''))\n",
        "        features['domain_with_copyright'] = int('copyright' in (soup.find('body').text if soup.find('body') else '').lower())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        # Set defaults for features related to URL fetching\n",
        "        keys = ['nb_hyperlinks', 'ratio_extHyperlinks', 'ratio_intHyperlinks', 'ratio_nullHyperlinks',\n",
        "                'nb_redirection', 'nb_external_redirection', 'ratio_intRedirection', 'ratio_extRedirection',\n",
        "                'links_in_tags', 'external_favicon', 'iframe', 'popup_window', 'sfh', 'safe_anchor',\n",
        "                'onmouseover', 'right_clic', 'empty_title', 'domain_in_title', 'domain_with_copyright']\n",
        "        for key in keys:\n",
        "            features[key] = 0\n",
        "\n",
        "    # Calculate the lengths of words\n",
        "    path_words = re.findall(r'\\w+', parsed_url.path)\n",
        "    host_words = re.findall(r'\\w+', parsed_url.hostname)\n",
        "    all_words = path_words + host_words\n",
        "    features['length_words_raw'] = len(' '.join(all_words))\n",
        "\n",
        "    features['char_repeat'] = len({char: url.count(char) for char in set(url) if url.count(char) > 1})\n",
        "\n",
        "    # Capture lengths of words\n",
        "    features['shortest_word_host'] = len(min(host_words, key=len, default=''))\n",
        "    features['shortest_word_path'] = len(min(path_words, key=len, default=''))\n",
        "    features['longest_word_host'] = len(max(host_words, key=len, default=''))\n",
        "    features['longest_word_path'] = len(max(path_words, key=len, default=''))\n",
        "\n",
        "    features['shortest_words_raw'] = len(min(all_words, key=len, default=''))\n",
        "    features['longest_words_raw'] = len(max(all_words, key=len, default=''))\n",
        "    features['avg_word_host'] = np.mean([len(word) for word in host_words]) if host_words else 0\n",
        "    features['avg_word_path'] = np.mean([len(word) for word in path_words]) if path_words else 0\n",
        "    features['avg_words_raw'] = np.mean([len(word) for word in all_words]) if all_words else 0\n",
        "\n",
        "    features['phish_hints'] = int(any(hint in url for hint in ['secure', 'login', 'update', 'verify']))\n",
        "\n",
        "    features['domain_in_brand'] = int(domain_info.domain in url)\n",
        "    features['brand_in_subdomain'] = int(any(brand in parsed_url.hostname for brand in ['brand', 'company']))\n",
        "    features['brand_in_path'] = int(any(brand in parsed_url.path for brand in ['brand', 'company']))\n",
        "\n",
        "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq']\n",
        "    features['suspecious_tld'] = int(any(tld in parsed_url.hostname for tld in suspicious_tlds))\n",
        "\n",
        "    # Placeholder for Int Errors and Ext Errors\n",
        "    features['ratio_intErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['ratio_extErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['submit_email'] = int(bool(soup.find('input', {'type': 'email'})))\n",
        "\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        features['whois_registered_domain'] = int(bool(whois_info.domain_name))\n",
        "        if whois_info.creation_date and whois_info.expiration_date:\n",
        "            if isinstance(whois_info.creation_date, list):\n",
        "                creation_date = whois_info.creation_date[0]\n",
        "            else:\n",
        "                creation_date = whois_info.creation_date\n",
        "\n",
        "            if isinstance(whois_info.expiration_date, list):\n",
        "                expiration_date = whois_info.expiration_date[0]\n",
        "            else:\n",
        "                expiration_date = whois_info.expiration_date\n",
        "\n",
        "            features['domain_registration_length'] = (expiration_date - creation_date).days\n",
        "            features['domain_age'] = (datetime.datetime.now() - creation_date).days\n",
        "        else:\n",
        "            features['domain_registration_length'] = -1\n",
        "            features['domain_age'] = -1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching WHOIS info: {e}\")\n",
        "        features['whois_registered_domain'] = 0\n",
        "        features['domain_registration_length'] = -1\n",
        "        features['domain_age'] = -1\n",
        "\n",
        "    return features\n",
        "\n",
        "# Load and preprocess data\n",
        "data_path = '/content/drive/MyDrive/dataset_phishing.csv'  # Adjust this to your data path\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Map the 'status' column to numeric values before splitting the data\n",
        "df['status'] = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
        "\n",
        "# Feature and target selection\n",
        "feature_columns = [ 'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn',\n",
        "    'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
        "    'ratio_digits_host', 'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain', 'nb_subdomains',\n",
        "    'prefix_suffix', 'random_domain', 'shortening_service', 'path_extension', 'nb_redirection', 'nb_external_redirection',\n",
        "    'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path',\n",
        "    'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path',\n",
        "    'phish_hints', 'domain_in_brand', 'brand_in_subdomain', 'brand_in_path', 'suspecious_tld',\n",
        "    'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS',\n",
        "    'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors', 'login_form',\n",
        "    'external_favicon', 'links_in_tags', 'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe',\n",
        "    'popup_window', 'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title',\n",
        "    'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length', 'domain_age']\n",
        "\n",
        "# Selecting features\n",
        "X = df[feature_columns].values\n",
        "y = df['status'].values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train.unsqueeze(1))  # Reshape for RNN\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train.unsqueeze(1)).round()\n",
        "    y_pred_test = model(X_test.unsqueeze(1)).round()\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Predicting new URL\n",
        "new_url = 'https://www.shadetreetechnology.com/V4/validation/ba4b8bddd7958ecb8772c836c2969531'  # Change this to any URL you want to test\n",
        "new_features = extract_features(new_url)\n",
        "\n",
        "# Extract feature values into a list\n",
        "feature_values_list = list(new_features.values())\n",
        "\n",
        "# If you want to convert the list to a DataFrame for scaling\n",
        "new_features_df = pd.DataFrame([feature_values_list])\n",
        "new_features_scaled = scaler.transform(new_features_df)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "new_features_tensor = torch.tensor(new_features_scaled, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Make prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    new_prediction = model(new_features_tensor).round()\n",
        "    print(f'Prediction for {new_url}: {\"Phishing\" if new_prediction.item() == 1 else \"Legitimate\"}')\n",
        "\n",
        "# Print extracted feature values list\n",
        "print(f'Extracted feature values: {feature_values_list}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "Cq0_UthekL5z",
        "outputId": "13521284-96e6-4a67-9f98-eb7cb82c2555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.2766\n",
            "Epoch [20/100], Loss: 0.2393\n",
            "Epoch [30/100], Loss: 0.2263\n",
            "Epoch [40/100], Loss: 0.2139\n",
            "Epoch [50/100], Loss: 0.1987\n",
            "Epoch [60/100], Loss: 0.1775\n",
            "Epoch [70/100], Loss: 0.1535\n",
            "Epoch [80/100], Loss: 0.1298\n",
            "Epoch [90/100], Loss: 0.1120\n",
            "Epoch [100/100], Loss: 0.0933\n",
            "Training Accuracy: 0.9682\n",
            "Test Accuracy: 0.9418\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.94      0.94      1157\n",
            "         1.0       0.94      0.94      0.94      1129\n",
            "\n",
            "    accuracy                           0.94      2286\n",
            "   macro avg       0.94      0.94      0.94      2286\n",
            "weighted avg       0.94      0.94      0.94      2286\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1091   66]\n",
            " [  67 1062]]\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 80 features, but StandardScaler is expecting 82 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-6aaf7b880b1a>\u001b[0m in \u001b[0;36m<cell line: 330>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;31m# If you want to convert the list to a DataFrame for scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0mnew_features_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_values_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m \u001b[0mnew_features_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;31m# Convert to PyTorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 80 features, but StandardScaler is expecting 82 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas requests beautifulsoup4 python-whois tldextract torch scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzvM1DypwIQM",
        "outputId": "19dba0db-24af-4aea-c472-bd168a2357a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading python_whois-0.9.4-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, python-whois, tldextract\n",
            "Successfully installed python-whois-0.9.4 requests-file-2.1.0 tldextract-5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import whois\n",
        "import tldextract\n",
        "import datetime\n",
        "import socket\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import joblib\n",
        "def is_valid_ip(ip):\n",
        "    \"\"\"Check if the hostname is an IP address.\"\"\"\n",
        "    try:\n",
        "        socket.inet_aton(ip)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Basic URL parsing\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Basic URL Length Features\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.hostname or '')\n",
        "\n",
        "    # Token Counts\n",
        "    tokens = ['.', '-', '@', '?', '&', '|', '=', '_', '~', '%', '/', '*', ':', ',', ';', '$', ' ']\n",
        "    token_names = ['nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore',\n",
        "                   'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma',\n",
        "                   'nb_semicolumn', 'nb_dollar', 'nb_space']\n",
        "\n",
        "    for token, name in zip(tokens, token_names):\n",
        "        features[name] = url.count(token)\n",
        "\n",
        "    features['nb_www'] = url.count('www')\n",
        "    features['nb_com'] = url.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    # Path Features\n",
        "    features['http_in_path'] = int('http' in parsed_url.path)\n",
        "    features['https_token'] = int('https' in url)\n",
        "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0\n",
        "\n",
        "    path_extensions = ['.php', '.asp', '.aspx', '.jsp']\n",
        "    features['path_extension'] = int(any(ext in parsed_url.path for ext in path_extensions))\n",
        "    features['prefix_suffix'] = int(any(part in url for part in ['.htm', '.html']))\n",
        "    random_strings = ['random', '1234', 'xyz']\n",
        "    features['random_domain'] = int(any(sub in parsed_url.hostname for sub in random_strings)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Domain Features\n",
        "    features['punycode'] = int(parsed_url.hostname.encode('idna').decode('utf-8') != parsed_url.hostname) if parsed_url.hostname else 0\n",
        "    shortening_services = ['bit.ly', 'tinyurl.com', 'goo.gl']\n",
        "    features['shortening_service'] = int(any(service in url for service in shortening_services))\n",
        "    features['port'] = parsed_url.port if parsed_url.port else -1\n",
        "    tlds = ['.com', '.net', '.org']\n",
        "    features['tld_in_path'] = int(any(tld in parsed_url.path for tld in tlds))\n",
        "    features['tld_in_subdomain'] = int(domain_info.suffix in parsed_url.hostname) if parsed_url.hostname else 0\n",
        "\n",
        "    # Check if IP address is used as hostname\n",
        "    features['ip'] = int(is_valid_ip(parsed_url.hostname)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Ratio of digits in hostname\n",
        "    features['ratio_digits_host'] = sum(c.isdigit() for c in parsed_url.hostname) / len(parsed_url.hostname) if parsed_url.hostname and len(parsed_url.hostname) > 0 else 0\n",
        "\n",
        "    # Abnormal subdomain\n",
        "    features['abnormal_subdomain'] = int(len(domain_info.subdomain.split('.')) > 2) if domain_info.subdomain else 0\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['nb_subdomains'] = len(domain_info.subdomain.split('.')) if domain_info.subdomain else 0\n",
        "\n",
        "    # Initialize media ratios\n",
        "    features['ratio_intMedia'] = 0\n",
        "    features['ratio_extMedia'] = 0\n",
        "\n",
        "    # Default values for URL content-based features\n",
        "    default_features = {\n",
        "        'nb_hyperlinks': 0,\n",
        "        'ratio_extHyperlinks': 0,\n",
        "        'ratio_intHyperlinks': 0,\n",
        "        'ratio_nullHyperlinks': 0,\n",
        "        'nb_redirection': 0,\n",
        "        'nb_external_redirection': 0,\n",
        "        'ratio_intRedirection': 0,\n",
        "        'ratio_extRedirection': 0,\n",
        "        'links_in_tags': 0,\n",
        "        'external_favicon': 0,\n",
        "        'iframe': 0,\n",
        "        'popup_window': 0,\n",
        "        'sfh': 0,\n",
        "        'safe_anchor': 0,\n",
        "        'onmouseover': 0,\n",
        "        'right_clic': 0,\n",
        "        'empty_title': 0,\n",
        "        'domain_in_title': 0,\n",
        "        'domain_with_copyright': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Count Links in the Page\n",
        "        links = soup.find_all('a')\n",
        "        total_links = len(links)\n",
        "        features['nb_hyperlinks'] = total_links\n",
        "\n",
        "        if total_links > 0:\n",
        "            ext_links = len([link for link in links if link.get('href', '').startswith('http')])\n",
        "            int_links = len([link for link in links if not link.get('href', '').startswith('http')])\n",
        "            null_links = len([link for link in links if not link.get('href')])\n",
        "\n",
        "            features['ratio_extHyperlinks'] = ext_links / total_links\n",
        "            features['ratio_intHyperlinks'] = int_links / total_links\n",
        "            features['ratio_nullHyperlinks'] = null_links / total_links\n",
        "        else:\n",
        "            features['ratio_extHyperlinks'] = 0\n",
        "            features['ratio_intHyperlinks'] = 0\n",
        "            features['ratio_nullHyperlinks'] = 0\n",
        "\n",
        "        # Calculate redirections\n",
        "        features['nb_redirection'] = len(response.history)\n",
        "        features['nb_external_redirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc != parsed_url.netloc])\n",
        "\n",
        "        # Internal and external redirection ratios\n",
        "        features['ratio_intRedirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc == parsed_url.netloc]) / len(response.history) if response.history else 0\n",
        "        features['ratio_extRedirection'] = features['nb_external_redirection'] / len(response.history) if response.history else 0\n",
        "\n",
        "        # Check for favicon\n",
        "        favicon_tag = soup.find('link', rel='shortcut icon')\n",
        "        features['external_favicon'] = int(favicon_tag and parsed_url.netloc not in favicon_tag.get('href', ''))\n",
        "\n",
        "        # Links in specific tags\n",
        "        script_links = len(soup.find_all('script', src=True))\n",
        "        link_links = len(soup.find_all('link', href=True))\n",
        "        features['links_in_tags'] = script_links + link_links\n",
        "\n",
        "        media_links = soup.find_all(['img', 'video', 'audio'])\n",
        "        features['ratio_extMedia'] = len(media_links) / len(links) if len(links) > 0 else 0\n",
        "        features['iframe'] = int(bool(soup.find('iframe')))\n",
        "        features['popup_window'] = int(bool(soup.find('script', string=re.compile('window.open'))))\n",
        "\n",
        "        safe_anchors = [link.get('rel') != 'nofollow' for link in links]\n",
        "        features['sfh'] = int(any(form.get('action', '').startswith('http') for form in soup.find_all('form')))\n",
        "        features['safe_anchor'] = int(all(safe_anchors))\n",
        "\n",
        "        features['onmouseover'] = int(bool(soup.find_all(attrs={\"onmouseover\": True})))\n",
        "        features['right_clic'] = int(bool(soup.find_all(attrs={\"oncontextmenu\": True})))\n",
        "        features['empty_title'] = int(bool(soup.find('title') and not soup.find('title').text.strip()))\n",
        "        features['domain_in_title'] = int(domain_info.domain in (soup.find('title').text if soup.find('title') else ''))\n",
        "        features['domain_with_copyright'] = int('copyright' in (soup.find('body').text if soup.find('body') else '').lower())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        features.update(default_features)\n",
        "\n",
        "    # Calculate the lengths of words\n",
        "    path_words = re.findall(r'\\w+', parsed_url.path)\n",
        "    host_words = re.findall(r'\\w+', parsed_url.hostname) if parsed_url.hostname else []\n",
        "    all_words = path_words + host_words\n",
        "    features['length_words_raw'] = len(' '.join(all_words))\n",
        "\n",
        "    features['char_repeat'] = len({char: url.count(char) for char in set(url) if url.count(char) > 1})\n",
        "\n",
        "    # Capture lengths of words\n",
        "    features['shortest_word_host'] = len(min(host_words, key=len, default=''))\n",
        "    features['shortest_word_path'] = len(min(path_words, key=len, default=''))\n",
        "    features['longest_word_host'] = len(max(host_words, key=len, default=''))\n",
        "    features['longest_word_path'] = len(max(path_words, key=len, default=''))\n",
        "\n",
        "    features['shortest_words_raw'] = len(min(all_words, key=len, default=''))\n",
        "    features['longest_words_raw'] = len(max(all_words, key=len, default=''))\n",
        "    features['avg_word_host'] = np.mean([len(word) for word in host_words]) if host_words else 0\n",
        "    features['avg_word_path'] = np.mean([len(word) for word in path_words]) if path_words else 0\n",
        "    features['avg_words_raw'] = np.mean([len(word) for word in all_words]) if all_words else 0\n",
        "\n",
        "    features['phish_hints'] = int(any(hint in url for hint in ['secure', 'login', 'update', 'verify']))\n",
        "\n",
        "    features['domain_in_brand'] = int(domain_info.domain in url)\n",
        "    features['brand_in_subdomain'] = int(any(brand in parsed_url.hostname for brand in ['brand', 'company'])) if parsed_url.hostname else 0\n",
        "    features['brand_in_path'] = int(any(brand in parsed_url.path for brand in ['brand', 'company']))\n",
        "\n",
        "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq']\n",
        "    features['suspecious_tld'] = int(any(tld in parsed_url.hostname for tld in suspicious_tlds)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Placeholder for Int Errors and Ext Errors\n",
        "    features['ratio_intErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['ratio_extErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['submit_email'] = int(bool(soup.find('input', {'type': 'email'}))) if 'soup' in locals() else 0\n",
        "\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        features['whois_registered_domain'] = int(bool(whois_info.domain_name))\n",
        "        if whois_info.creation_date and whois_info.expiration_date:\n",
        "            if isinstance(whois_info.creation_date, list):\n",
        "                creation_date = whois_info.creation_date[0]\n",
        "            else:\n",
        "                creation_date = whois_info.creation_date\n",
        "\n",
        "            if isinstance(whois_info.expiration_date, list):\n",
        "                expiration_date = whois_info.expiration_date[0]\n",
        "            else:\n",
        "                expiration_date = whois_info.expiration_date\n",
        "\n",
        "            features['domain_registration_length'] = (expiration_date - creation_date).days\n",
        "            features['domain_age'] = (datetime.datetime.now() - creation_date).days\n",
        "        else:\n",
        "            features['domain_registration_length'] = -1\n",
        "            features['domain_age'] = -1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching WHOIS info: {e}\")\n",
        "        features['whois_registered_domain'] = 0\n",
        "        features['domain_registration_length'] = -1\n",
        "        features['domain_age'] = -1\n",
        "\n",
        "    return features\n",
        "\n",
        "# Load and preprocess data\n",
        "data_path = '/content/drive/MyDrive/dataset_phishing.csv'  # Adjust this to your data path\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# Map the 'status' column to numeric values before splitting the data\n",
        "df['status'] = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
        "\n",
        "# Feature and target selection\n",
        "feature_columns = [\n",
        "     'url', 'length_url', 'length_hostname',  # Added 'url' as a feature temporarily\n",
        "    'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon',\n",
        "    'nb_comma', 'nb_semicolumn', 'nb_dollar', 'nb_space', 'nb_www', 'nb_com',\n",
        "    'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url', 'ratio_digits_host',\n",
        "    'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain',\n",
        "    'nb_subdomains', 'prefix_suffix', 'random_domain', 'shortening_service',\n",
        "    'path_extension', 'nb_redirection', 'nb_external_redirection', 'length_words_raw',\n",
        "    'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path',\n",
        "    'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw',\n",
        "    'avg_word_host', 'avg_word_path', 'phish_hints', 'domain_in_brand',\n",
        "    'brand_in_subdomain', 'brand_in_path', 'suspecious_tld', 'nb_hyperlinks',\n",
        "    'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks',\n",
        "    'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors',\n",
        "  'external_favicon', 'links_in_tags', 'submit_email',\n",
        "    'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe', 'popup_window',\n",
        "    'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title',\n",
        "    'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length',\n",
        "    'domain_age'\n",
        "]\n",
        "\n",
        "# Selecting features without including 'url' for model training\n",
        "X = df[feature_columns].drop(columns=['url']).values  # Exclude the 'url' column\n",
        "y = df['status'].values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train.unsqueeze(1))  # Reshape for RNN\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train.unsqueeze(1)).round()\n",
        "    y_pred_test = model(X_test.unsqueeze(1)).round()\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Predicting new URL\n",
        "urls = [\n",
        "    'https://now-get-free-verified.vercel.app/',\n",
        "    'https://s.monsoondealz.xyz/',\n",
        "    'http://dpd-hr.receiving-delivery.com/track/5387861827/',\n",
        "    'https://www.google.com/,',\n",
        "    'https://www.facebook.com/',\n",
        "    'http://rgipt.ac.in',\n",
        "    'https://www.youtube.com/',\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Extract features for each URL\n",
        "feature_values_list = []\n",
        "for url in urls:\n",
        "    new_features = extract_features(url)\n",
        "\n",
        "    # Extract feature values into a list\n",
        "    feature_values = [new_features[col] for col in feature_columns if col != 'url']\n",
        "\n",
        "    # Append to the list\n",
        "    feature_values_list.append(feature_values)\n",
        "\n",
        "# Create DataFrame for the new features\n",
        "new_features_df = pd.DataFrame(feature_values_list, columns=[col for col in feature_columns if col != 'url'])\n",
        "\n",
        "# Scale the features\n",
        "new_features_scaled = scaler.transform(new_features_df)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "new_features_tensor = torch.tensor(new_features_scaled, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(new_features_tensor).round()\n",
        "\n",
        "# Print predictions for each URL\n",
        "for url, pred in zip(urls, predictions):\n",
        "    print(f'Prediction for {url}: {\"Phishing\" if pred.item() == 1 else \"Legitimate\"}')\n",
        "\n",
        "# Print extracted feature values list for debugging\n",
        "for url, features in zip(urls, feature_values_list):\n",
        "    print(f'Extracted feature values for {url}: {features}')\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/sweeb11.pth'\n",
        "scaler_path = '/content/drive/MyDrive/sweb12.pkl'\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"Scaler saved to {scaler_path}\")\n",
        "\n",
        "# Confirm the files exist\n",
        "import os\n",
        "print(\"Files in drive:\")\n",
        "print(os.listdir('/content/drive/MyDrive'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXBRnBoHj6G8",
        "outputId": "e35eee22-1b90-4770-e8c7-9d3da5cf6a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.2767\n",
            "Epoch [20/100], Loss: 0.2407\n",
            "Epoch [30/100], Loss: 0.2288\n",
            "Epoch [40/100], Loss: 0.2198\n",
            "Epoch [50/100], Loss: 0.2087\n",
            "Epoch [60/100], Loss: 0.1921\n",
            "Epoch [70/100], Loss: 0.1704\n",
            "Epoch [80/100], Loss: 0.1467\n",
            "Epoch [90/100], Loss: 0.1243\n",
            "Epoch [100/100], Loss: 0.1036\n",
            "Training Accuracy: 0.9612\n",
            "Test Accuracy: 0.9396\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.93      0.94      1157\n",
            "         1.0       0.93      0.95      0.94      1129\n",
            "\n",
            "    accuracy                           0.94      2286\n",
            "   macro avg       0.94      0.94      0.94      2286\n",
            "weighted avg       0.94      0.94      0.94      2286\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1079   78]\n",
            " [  60 1069]]\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Error fetching URL content: HTTPSConnectionPool(host='s.monsoondealz.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7b9887875360>: Failed to resolve 's.monsoondealz.xyz' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching URL content: HTTPConnectionPool(host='dpd-hr.receiving-delivery.com', port=80): Max retries exceeded with url: /track/5387861827/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7b98878777f0>: Failed to resolve 'dpd-hr.receiving-delivery.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Prediction for https://now-get-free-verified.vercel.app/: Phishing\n",
            "Prediction for https://s.monsoondealz.xyz/: Phishing\n",
            "Prediction for http://dpd-hr.receiving-delivery.com/track/5387861827/: Phishing\n",
            "Prediction for https://www.google.com/,: Legitimate\n",
            "Prediction for https://www.facebook.com/: Legitimate\n",
            "Prediction for http://rgipt.ac.in: Legitimate\n",
            "Prediction for https://www.youtube.com/: Legitimate\n",
            "Extracted feature values for https://now-get-free-verified.vercel.app/: [41, 32, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 32, 10, 3, 3, 0, 8, 8, 0, 4.5, 4.5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1]\n",
            "Extracted feature values for https://s.monsoondealz.xyz/: [27, 18, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 18, 7, 1, 1, 0, 12, 12, 0, 5.333333333333333, 5.333333333333333, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 365, 27]\n",
            "Extracted feature values for http://dpd-hr.receiving-delivery.com/track/5387861827/: [54, 29, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0.18518518518518517, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 46, 14, 2, 2, 5, 10, 9, 10, 5.714285714285714, 5.0, 7.5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 365, 53]\n",
            "Extracted feature values for https://www.google.com/,: [24, 14, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 14, 6, 3, 3, 0, 6, 6, 0, 4.0, 4.0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 11322, 9861]\n",
            "Extracted feature values for https://www.facebook.com/: [25, 16, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 16, 6, 3, 3, 0, 8, 8, 0, 4.666666666666667, 4.666666666666667, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 13149, 10031]\n",
            "Extracted feature values for http://rgipt.ac.in: [18, 11, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0.0, 0.0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 11, 5, 2, 2, 0, 5, 5, 0, 3.0, 3.0, 0, 0, 1, 0, 0, 0, 67, 0.8507462686567164, 0.14925373134328357, 0.08955223880597014, 1.0, 0.0, 0, 0, 1, 33, 0, 0, 0.4626865671641791, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 6575, 6199]\n",
            "Extracted feature values for https://www.youtube.com/: [24, 15, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 15, 6, 3, 3, 0, 7, 7, 0, 4.333333333333333, 4.333333333333333, 0, 0, 1, 0, 0, 0, 15, 0.4, 0.6, 0.0, 0, 0, 0, 0, 0, 27, 0, 0, 0.0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 7305, 7151]\n",
            "Model saved to /content/drive/MyDrive/sweeb11.pth\n",
            "Scaler saved to /content/drive/MyDrive/sweb12.pkl\n",
            "Files in drive:\n",
            "['Untitled form (File responses)', 'Untitled form (Responses).gsheet', 'TITAN ESPORTS CLAN REGESTRATION FORM  (File responses)', 'dataset_phishing.csv', 'IMG-20210705-WA0031 (1).jpg', 'IMG-20210705-WA0030.jpg', 'IMG-20210705-WA0031.jpg', 'WhatsApp Image 2021-07-05 at 10.21.44.jpeg', 'WhatsApp Image 2021-07-05 at 10.21.46.jpeg', 'WhatsApp Image 2021-07-05 at 10.37.27.jpeg', 'WhatsApp Image 2021-07-05 at 10.39.40.jpeg', 'Untitled form (5).gform', 'images.jpeg', 'Unlock 90FPS PUBG + BGMI all versions(2).zip', 'Untitled form (4).gform', 'Untitled form (3).gform', 'Copy of Untitled form.gform', 'Untitled form (2).gform', 'Employee happiness.pptx', 'Untitled form (1).gform', 'color_names.csv', '20220514_101731 (1).png', '20220514_101731.png', 'GJ13 2223 2285.pdf', '3350702_DWPD_GTU_Study_Material_e-Notes_Unit-5_22092020071825AM.gdoc', 'DI-SEM5REGULARWINTEREXAMINATION-2022_206330307002.pdf', 'dataset.csv', 'vcards_20221123_180126.vcf', 'Untitled folder', 'bibology.gdoc', 'sem 5', 'hello.gdoc', '2022 sales.gdoc', 'Untitled document (4).gdoc', 'sales.gdoc', 'zkj-dxxz-hrt – 22 Jan 2023.gjam', 'midj propompt.gdoc', 'midj1.gdoc', 'GOLU MARRIGE', 'Advanced ui Student', 'Advanced ui admin', 'Adavnced ui Teacher', 'oldui', 'Auth form', 'Auth form updated', 'Untitled document (3).gdoc', 'Project_My-Awesome-Project.txt', 'Untitled form.gform', '\\\\\\\\.gform', 'nma', 'nma 10 to 20', 'p13_7019', 'Untitled document (2).gdoc', 'Untitled document (1).gdoc', 'p10.gdoc', 'gg', 'Wawa Logo.jpg', 'op.gsheet', 'Untitled spreadsheet (2).gsheet', 'HyperWrite Personal Assistant Test.gdoc', '4_5929477208560110997.gdoc', 'Translated copy of 4_5929477208560110997.gdoc', 'React native 75', 'Screenshot_2024-02-07-13-39-59-77_49b96b5fbae0d12a18edc4a3afe0dfd9.jpg', 'Screenshot_2024-03-06-23-18-48-85_49b96b5fbae0d12a18edc4a3afe0dfd9.jpg', 'Google hackathon.pdf', 'Untitled spreadsheet (1).gsheet', 'Colab Notebooks', 'color_names.gsheet', 'csvjson.jsonl', 'spam.csv', 'scam.csv', 'spam.gsheet', 'scam1.csv', 'scam2.numbers', 'scam2.csv.gsheet', 'scam2.csv', 'scam2.gsheet', 'scam3.csv', 'appsheet', 'scam3.gsheet', 'scam.jsonl', 'scam4.csv', 'scam4 (1).gsheet', 'scam5.csv', 'Copy of scam4.gsheet', 'scam6.csv', 'scams.pt', 'scams.pth', 'vect.pkl', 'Computer Networks (CN) (Professional Core) 5.gdoc', 'Professional ethics (PE) (Humanities and Social Science) 3.gdoc', 'Analysis and Design of Algorithms (ADA) (Professional Core) 5.gdoc', 'Software Engineering (SE)(Professional Elective – I) 4.gdoc', 'Python for Data Science (PDS) (Open Elective - I) 3.gdoc', 'scam7.csv', 'scam7.gsheet', 'scamss.pth', 'vectt.pkl', 'scamsss.pth', 'vecttt.pkl', 'scam1.pth', 'vect1.pkl', 'scams1.pth', 'vects.pkl', 'scam_detection_model.pth', 'tfidf_vectorizer.pkl', 'Untitled spreadsheet.gsheet', 'best_scam_detection_model.pth', 'best_sms_scam_detection_model.pth', 'sms_tfidf_vectorizer.pkl', 'sgmail.gsheet', 'sgmail.csv', 'gmail_tfidf_vectorizer.pkl', 'sgmails.csv', 'Copy of sgmails.csv', 'SGMAILS.csv', 'Copy of sgmails (1).gsheet', 'Copy of sgmails.gsheet', 'SGMAILSs.gsheet', 'SGMAILSscsv.csv', 'best_improved_gmail_scam_detection_model.pth', 'best_gmail_scam_detection_model.pth', 'gmail_hash_vectorizer.pkl', 'Untitled document.gdoc', 'verified_online.csv', 'verified_online.gsheet', 'dataset_phishing (2).gsheet', '5k.csv', 'phishing.csv', 'dataset_phishing (1).gsheet', 'dataset_phishing.gsheet', 'sweb.pkl', 'sweb.pth', 'sweeb11.pth', 'sweb12.pkl', 'aisc.csv', 'aisccc.csv', 'aicc.csv', 'aisc.gsheet', 'aisccc.gsheet', 'ais.csv', 'aics.csv', 'scam4.gsheet', 'aiscl.csv', 'aiscll.csv', 'smaj']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import whois\n",
        "import tldextract\n",
        "import datetime\n",
        "import socket\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import joblib\n",
        "\n",
        "def is_valid_ip(ip):\n",
        "    \"\"\"Check if the hostname is an IP address.\"\"\"\n",
        "    try:\n",
        "        socket.inet_aton(ip)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Basic URL parsing\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Basic URL Length Features\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.hostname or '')\n",
        "\n",
        "    # Token Counts\n",
        "    tokens = ['.', '-', '@', '?', '&', '|', '=', '_', '~', '%', '/', '*', ':', ',', ';', '$', ' ']\n",
        "    token_names = ['nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore',\n",
        "                   'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma',\n",
        "                   'nb_semicolumn', 'nb_dollar', 'nb_space']\n",
        "\n",
        "    for token, name in zip(tokens, token_names):\n",
        "        features[name] = url.count(token)\n",
        "\n",
        "    features['nb_www'] = url.count('www')\n",
        "    features['nb_com'] = url.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    # Path Features\n",
        "    features['http_in_path'] = int('http' in parsed_url.path)\n",
        "    features['https_token'] = int('https' in url)\n",
        "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0\n",
        "\n",
        "    path_extensions = ['.php', '.asp', '.aspx', '.jsp']\n",
        "    features['path_extension'] = int(any(ext in parsed_url.path for ext in path_extensions))\n",
        "    features['prefix_suffix'] = int(any(part in url for part in ['.htm', '.html']))\n",
        "    random_strings = ['random', '1234', 'xyz']\n",
        "    features['random_domain'] = int(any(sub in parsed_url.hostname for sub in random_strings)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Domain Features\n",
        "    features['punycode'] = int(parsed_url.hostname.encode('idna').decode('utf-8') != parsed_url.hostname) if parsed_url.hostname else 0\n",
        "    shortening_services = ['bit.ly', 'tinyurl.com', 'goo.gl']\n",
        "    features['shortening_service'] = int(any(service in url for service in shortening_services))\n",
        "    features['port'] = parsed_url.port if parsed_url.port else -1\n",
        "    tlds = ['.com', '.net', '.org']\n",
        "    features['tld_in_path'] = int(any(tld in parsed_url.path for tld in tlds))\n",
        "    features['tld_in_subdomain'] = int(domain_info.suffix in parsed_url.hostname) if parsed_url.hostname else 0\n",
        "\n",
        "    # Check if IP address is used as hostname\n",
        "    features['ip'] = int(is_valid_ip(parsed_url.hostname)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Ratio of digits in hostname\n",
        "    features['ratio_digits_host'] = sum(c.isdigit() for c in parsed_url.hostname) / len(parsed_url.hostname) if parsed_url.hostname and len(parsed_url.hostname) > 0 else 0\n",
        "\n",
        "    # Abnormal subdomain\n",
        "    features['abnormal_subdomain'] = int(len(domain_info.subdomain.split('.')) > 2) if domain_info.subdomain else 0\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['nb_subdomains'] = len(domain_info.subdomain.split('.')) if domain_info.subdomain else 0\n",
        "\n",
        "    # Initialize media ratios\n",
        "    features['ratio_intMedia'] = 0\n",
        "    features['ratio_extMedia'] = 0\n",
        "\n",
        "    # Default values for URL content-based features\n",
        "    default_features = {\n",
        "        'nb_hyperlinks': 0,\n",
        "        'ratio_extHyperlinks': 0,\n",
        "        'ratio_intHyperlinks': 0,\n",
        "        'ratio_nullHyperlinks': 0,\n",
        "        'nb_redirection': 0,\n",
        "        'nb_external_redirection': 0,\n",
        "        'ratio_intRedirection': 0,\n",
        "        'ratio_extRedirection': 0,\n",
        "        'links_in_tags': 0,\n",
        "        'external_favicon': 0,\n",
        "        'iframe': 0,\n",
        "        'popup_window': 0,\n",
        "        'sfh': 0,\n",
        "        'safe_anchor': 0,\n",
        "        'onmouseover': 0,\n",
        "        'right_clic': 0,\n",
        "        'empty_title': 0,\n",
        "        'domain_in_title': 0,\n",
        "        'domain_with_copyright': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Count Links in the Page\n",
        "        links = soup.find_all('a')\n",
        "        total_links = len(links)\n",
        "        features['nb_hyperlinks'] = total_links\n",
        "\n",
        "        if total_links > 0:\n",
        "            ext_links = len([link for link in links if link.get('href', '').startswith('http')])\n",
        "            int_links = len([link for link in links if not link.get('href', '').startswith('http')])\n",
        "            null_links = len([link for link in links if not link.get('href')])\n",
        "\n",
        "            features['ratio_extHyperlinks'] = ext_links / total_links\n",
        "            features['ratio_intHyperlinks'] = int_links / total_links\n",
        "            features['ratio_nullHyperlinks'] = null_links / total_links\n",
        "        else:\n",
        "            features['ratio_extHyperlinks'] = 0\n",
        "            features['ratio_intHyperlinks'] = 0\n",
        "            features['ratio_nullHyperlinks'] = 0\n",
        "\n",
        "        # Calculate redirections\n",
        "        features['nb_redirection'] = len(response.history)\n",
        "        features['nb_external_redirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc != parsed_url.netloc])\n",
        "\n",
        "        # Internal and external redirection ratios\n",
        "        features['ratio_intRedirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc == parsed_url.netloc]) / len(response.history) if response.history else 0\n",
        "        features['ratio_extRedirection'] = features['nb_external_redirection'] / len(response.history) if response.history else 0\n",
        "\n",
        "        # Check for favicon\n",
        "        favicon_tag = soup.find('link', rel='shortcut icon')\n",
        "        features['external_favicon'] = int(favicon_tag and parsed_url.netloc not in favicon_tag.get('href', ''))\n",
        "\n",
        "        # Links in specific tags\n",
        "        script_links = len(soup.find_all('script', src=True))\n",
        "        link_links = len(soup.find_all('link', href=True))\n",
        "        features['links_in_tags'] = script_links + link_links\n",
        "\n",
        "        media_links = soup.find_all(['img', 'video', 'audio'])\n",
        "        features['ratio_extMedia'] = len(media_links) / len(links) if len(links) > 0 else 0\n",
        "        features['iframe'] = int(bool(soup.find('iframe')))\n",
        "        features['popup_window'] = int(bool(soup.find('script', string=re.compile('window.open'))))\n",
        "\n",
        "        safe_anchors = [link.get('rel') != 'nofollow' for link in links]\n",
        "        features['sfh'] = int(any(form.get('action', '').startswith('http') for form in soup.find_all('form')))\n",
        "        features['safe_anchor'] = int(all(safe_anchors))\n",
        "\n",
        "        features['onmouseover'] = int(bool(soup.find_all(attrs={\"onmouseover\": True})))\n",
        "        features['right_clic'] = int(bool(soup.find_all(attrs={\"oncontextmenu\": True})))\n",
        "        features['empty_title'] = int(bool(soup.find('title') and not soup.find('title').text.strip()))\n",
        "        features['domain_in_title'] = int(domain_info.domain in (soup.find('title').text if soup.find('title') else ''))\n",
        "        features['domain_with_copyright'] = int('copyright' in (soup.find('body').text if soup.find('body') else '').lower())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        features.update(default_features)\n",
        "\n",
        "    # Calculate the lengths of words\n",
        "    path_words = re.findall(r'\\w+', parsed_url.path)\n",
        "    host_words = re.findall(r'\\w+', parsed_url.hostname) if parsed_url.hostname else []\n",
        "    all_words = path_words + host_words\n",
        "    features['length_words_raw'] = len(' '.join(all_words))\n",
        "\n",
        "    features['char_repeat'] = len({char: url.count(char) for char in set(url) if url.count(char) > 1})\n",
        "\n",
        "    # Capture lengths of words\n",
        "    features['shortest_word_host'] = len(min(host_words, key=len, default=''))\n",
        "    features['shortest_word_path'] = len(min(path_words, key=len, default=''))\n",
        "    features['longest_word_host'] = len(max(host_words, key=len, default=''))\n",
        "    features['longest_word_path'] = len(max(path_words, key=len, default=''))\n",
        "\n",
        "    features['shortest_words_raw'] = len(min(all_words, key=len, default=''))\n",
        "    features['longest_words_raw'] = len(max(all_words, key=len, default=''))\n",
        "    features['avg_word_host'] = np.mean([len(word) for word in host_words]) if host_words else 0\n",
        "    features['avg_word_path'] = np.mean([len(word) for word in path_words]) if path_words else 0\n",
        "    features['avg_words_raw'] = np.mean([len(word) for word in all_words]) if all_words else 0\n",
        "\n",
        "    features['phish_hints'] = int(any(hint in url for hint in ['secure', 'login', 'update', 'verify']))\n",
        "\n",
        "    features['domain_in_brand'] = int(domain_info.domain in url)\n",
        "    features['brand_in_subdomain'] = int(any(brand in parsed_url.hostname for brand in ['brand', 'company'])) if parsed_url.hostname else 0\n",
        "    features['brand_in_path'] = int(any(brand in parsed_url.path for brand in ['brand', 'company']))\n",
        "\n",
        "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq']\n",
        "    features['suspecious_tld'] = int(any(tld in parsed_url.hostname for tld in suspicious_tlds)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Placeholder for Int Errors and Ext Errors\n",
        "    features['ratio_intErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['ratio_extErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['submit_email'] = int(bool(soup.find('input', {'type': 'email'}))) if 'soup' in locals() else 0\n",
        "\n",
        "    features['whois_registered_domain'] = 0\n",
        "    features['domain_registration_length'] = -1\n",
        "\n",
        "    # Attempt to get WHOIS info directly if needed\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        features['whois_registered_domain'] = int(bool(whois_info.domain_name))\n",
        "        if whois_info.creation_date and whois_info.expiration_date:\n",
        "            if isinstance(whois_info.creation_date, list):\n",
        "                creation_date = whois_info.creation_date[0]\n",
        "            else:\n",
        "                creation_date = whois_info.creation_date\n",
        "\n",
        "            if isinstance(whois_info.expiration_date, list):\n",
        "                expiration_date = whois_info.expiration_date[0]\n",
        "            else:\n",
        "                expiration_date = whois_info.expiration_date\n",
        "\n",
        "            features['domain_registration_length'] = (expiration_date - creation_date).days\n",
        "        else:\n",
        "            features['domain_registration_length'] = -1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching WHOIS info: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Load and preprocess data\n",
        "data_path = '/content/drive/MyDrive/dataset_phishing.csv'  # Adjust this to your data path\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Map the 'status' column to numeric values before splitting the data\n",
        "df['status'] = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
        "\n",
        "# Feature and target selection\n",
        "feature_columns = [\n",
        "    'url', 'length_url', 'length_hostname',  # Added 'url' as a feature temporarily\n",
        "    'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon',\n",
        "    'nb_comma', 'nb_semicolumn', 'nb_dollar', 'nb_space', 'nb_www', 'nb_com',\n",
        "    'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url', 'ratio_digits_host',\n",
        "    'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain',\n",
        "    'nb_subdomains', 'prefix_suffix', 'random_domain', 'shortening_service',\n",
        "    'path_extension', 'nb_redirection', 'nb_external_redirection', 'length_words_raw',\n",
        "    'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path',\n",
        "    'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw',\n",
        "    'avg_word_host', 'avg_word_path', 'phish_hints', 'domain_in_brand',\n",
        "    'brand_in_subdomain', 'brand_in_path', 'suspecious_tld', 'nb_hyperlinks',\n",
        "    'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks',\n",
        "    'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors',\n",
        "    'external_favicon', 'links_in_tags', 'submit_email',\n",
        "    'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe', 'popup_window',\n",
        "    'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title',\n",
        "    'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length'\n",
        "]\n",
        "\n",
        "# Selecting features without including 'url' for model training\n",
        "X = df[feature_columns].drop(columns=['url']).values  # Exclude the 'url' column\n",
        "y = df['status'].values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "num_epochs = 100\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train.unsqueeze(1))  # Reshape for RNN\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train.unsqueeze(1)).round()\n",
        "    y_pred_test = model(X_test.unsqueeze(1)).round()\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "# Predicting new URL\n",
        "urls = [\n",
        "    'https://now-get-free-verified.vercel.app/',\n",
        "    'https://s.monsoondealz.xyz/',\n",
        "    'http://dpd-hr.receiving-delivery.com/track/5387861827/',\n",
        "    'https://www.google.com/',\n",
        "    'https://www.facebook.com/',\n",
        "    'http://rgipt.ac.in',\n",
        "    'https://www.youtube.com/',\n",
        "]\n",
        "\n",
        "# Extract features for each URL\n",
        "feature_values_list = []\n",
        "for url in urls:\n",
        "    new_features = extract_features(url)\n",
        "\n",
        "    # Extract feature values into a list\n",
        "    feature_values = [new_features[col] for col in feature_columns if col != 'url']\n",
        "\n",
        "    # Append to the list\n",
        "    feature_values_list.append(feature_values)\n",
        "\n",
        "# Create DataFrame for the new features\n",
        "new_features_df = pd.DataFrame(feature_values_list, columns=[col for col in feature_columns if col != 'url'])\n",
        "\n",
        "# Scale the features\n",
        "new_features_scaled = scaler.transform(new_features_df)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "new_features_tensor = torch.tensor(new_features_scaled, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(new_features_tensor).round()\n",
        "\n",
        "# Print predictions for each URL\n",
        "for url, pred in zip(urls, predictions):\n",
        "    print(f'Prediction for {url}: {\"Phishing\" if pred.item() == 1 else \"Legitimate\"}')\n",
        "\n",
        "# Print extracted feature values list for debugging\n",
        "for url, features in zip(urls, feature_values_list):\n",
        "    print(f'Extracted feature values for {url}: {features}')\n",
        "\n",
        "# Save the model and scaler\n",
        "model_path = '/content/drive/MyDrive/sweeb11.pth'\n",
        "scaler_path = '/content/drive/MyDrive/sweb12.pkl'\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"Scaler saved to {scaler_path}\")\n",
        "\n",
        "# Confirm the files exist\n",
        "import os\n",
        "print(\"Files in drive:\")\n",
        "print(os.listdir('/content/drive/MyDrive'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w6PggVxdkbF",
        "outputId": "e8cbcec3-100a-4906-b6bf-5da54ea834a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.3018\n",
            "Epoch [20/100], Loss: 0.2615\n",
            "Epoch [30/100], Loss: 0.2473\n",
            "Epoch [40/100], Loss: 0.2349\n",
            "Epoch [50/100], Loss: 0.2190\n",
            "Epoch [60/100], Loss: 0.1992\n",
            "Epoch [70/100], Loss: 0.1759\n",
            "Epoch [80/100], Loss: 0.1502\n",
            "Epoch [90/100], Loss: 0.1308\n",
            "Epoch [100/100], Loss: 0.1147\n",
            "Training Accuracy: 0.9611\n",
            "Test Accuracy: 0.9353\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.95      0.94      1157\n",
            "         1.0       0.95      0.92      0.93      1129\n",
            "\n",
            "    accuracy                           0.94      2286\n",
            "   macro avg       0.94      0.94      0.94      2286\n",
            "weighted avg       0.94      0.94      0.94      2286\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1098   59]\n",
            " [  89 1040]]\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Error fetching URL content: HTTPSConnectionPool(host='s.monsoondealz.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7b995831a3e0>: Failed to resolve 's.monsoondealz.xyz' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching URL content: HTTPConnectionPool(host='dpd-hr.receiving-delivery.com', port=80): Max retries exceeded with url: /track/5387861827/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7b9886ac9720>: Failed to resolve 'dpd-hr.receiving-delivery.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Prediction for https://now-get-free-verified.vercel.app/: Legitimate\n",
            "Prediction for https://s.monsoondealz.xyz/: Legitimate\n",
            "Prediction for http://dpd-hr.receiving-delivery.com/track/5387861827/: Legitimate\n",
            "Prediction for https://www.google.com/: Legitimate\n",
            "Prediction for https://www.facebook.com/: Legitimate\n",
            "Prediction for http://rgipt.ac.in: Legitimate\n",
            "Prediction for https://www.youtube.com/: Legitimate\n",
            "Extracted feature values for https://now-get-free-verified.vercel.app/: [41, 32, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 32, 10, 3, 3, 0, 8, 8, 0, 4.5, 4.5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n",
            "Extracted feature values for https://s.monsoondealz.xyz/: [27, 18, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 18, 7, 1, 1, 0, 12, 12, 0, 5.333333333333333, 5.333333333333333, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 365]\n",
            "Extracted feature values for http://dpd-hr.receiving-delivery.com/track/5387861827/: [54, 29, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0.18518518518518517, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 46, 14, 2, 2, 5, 10, 9, 10, 5.714285714285714, 5.0, 7.5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 365]\n",
            "Extracted feature values for https://www.google.com/: [23, 14, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 14, 6, 3, 3, 0, 6, 6, 0, 4.0, 4.0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 11322]\n",
            "Extracted feature values for https://www.facebook.com/: [25, 16, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 16, 6, 3, 3, 0, 8, 8, 0, 4.666666666666667, 4.666666666666667, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 13149]\n",
            "Extracted feature values for http://rgipt.ac.in: [18, 11, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0.0, 0.0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 11, 5, 2, 2, 0, 5, 5, 0, 3.0, 3.0, 0, 0, 1, 0, 0, 0, 67, 0.8507462686567164, 0.14925373134328357, 0.08955223880597014, 1.0, 0.0, 0, 0, 1, 33, 0, 0, 0.4626865671641791, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 6575]\n",
            "Extracted feature values for https://www.youtube.com/: [24, 15, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0.0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 15, 6, 3, 3, 0, 7, 7, 0, 4.333333333333333, 4.333333333333333, 0, 0, 1, 0, 0, 0, 15, 0.4, 0.6, 0.0, 0, 0, 0, 0, 0, 27, 0, 0, 0.0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 7305]\n",
            "Model saved to /content/drive/MyDrive/sweeb11.pth\n",
            "Scaler saved to /content/drive/MyDrive/sweb12.pkl\n",
            "Files in drive:\n",
            "['Untitled form (File responses)', 'Untitled form (Responses).gsheet', 'TITAN ESPORTS CLAN REGESTRATION FORM  (File responses)', 'dataset_phishing.csv', 'IMG-20210705-WA0031 (1).jpg', 'IMG-20210705-WA0030.jpg', 'IMG-20210705-WA0031.jpg', 'WhatsApp Image 2021-07-05 at 10.21.44.jpeg', 'WhatsApp Image 2021-07-05 at 10.21.46.jpeg', 'WhatsApp Image 2021-07-05 at 10.37.27.jpeg', 'WhatsApp Image 2021-07-05 at 10.39.40.jpeg', 'Untitled form (5).gform', 'images.jpeg', 'Unlock 90FPS PUBG + BGMI all versions(2).zip', 'Untitled form (4).gform', 'Untitled form (3).gform', 'Copy of Untitled form.gform', 'Untitled form (2).gform', 'Employee happiness.pptx', 'Untitled form (1).gform', 'color_names.csv', '20220514_101731 (1).png', '20220514_101731.png', 'GJ13 2223 2285.pdf', '3350702_DWPD_GTU_Study_Material_e-Notes_Unit-5_22092020071825AM.gdoc', 'DI-SEM5REGULARWINTEREXAMINATION-2022_206330307002.pdf', 'dataset.csv', 'vcards_20221123_180126.vcf', 'Untitled folder', 'bibology.gdoc', 'sem 5', 'hello.gdoc', '2022 sales.gdoc', 'Untitled document (4).gdoc', 'sales.gdoc', 'zkj-dxxz-hrt – 22 Jan 2023.gjam', 'midj propompt.gdoc', 'midj1.gdoc', 'GOLU MARRIGE', 'Advanced ui Student', 'Advanced ui admin', 'Adavnced ui Teacher', 'oldui', 'Auth form', 'Auth form updated', 'Untitled document (3).gdoc', 'Project_My-Awesome-Project.txt', 'Untitled form.gform', '\\\\\\\\.gform', 'nma', 'nma 10 to 20', 'p13_7019', 'Untitled document (2).gdoc', 'Untitled document (1).gdoc', 'p10.gdoc', 'gg', 'Wawa Logo.jpg', 'op.gsheet', 'Untitled spreadsheet (2).gsheet', 'HyperWrite Personal Assistant Test.gdoc', '4_5929477208560110997.gdoc', 'Translated copy of 4_5929477208560110997.gdoc', 'React native 75', 'Screenshot_2024-02-07-13-39-59-77_49b96b5fbae0d12a18edc4a3afe0dfd9.jpg', 'Screenshot_2024-03-06-23-18-48-85_49b96b5fbae0d12a18edc4a3afe0dfd9.jpg', 'Google hackathon.pdf', 'Untitled spreadsheet (1).gsheet', 'Colab Notebooks', 'color_names.gsheet', 'csvjson.jsonl', 'spam.csv', 'scam.csv', 'spam.gsheet', 'scam1.csv', 'scam2.numbers', 'scam2.csv.gsheet', 'scam2.csv', 'scam2.gsheet', 'scam3.csv', 'appsheet', 'scam3.gsheet', 'scam.jsonl', 'scam4.csv', 'scam4 (1).gsheet', 'scam5.csv', 'Copy of scam4.gsheet', 'scam6.csv', 'scams.pt', 'scams.pth', 'vect.pkl', 'Computer Networks (CN) (Professional Core) 5.gdoc', 'Professional ethics (PE) (Humanities and Social Science) 3.gdoc', 'Analysis and Design of Algorithms (ADA) (Professional Core) 5.gdoc', 'Software Engineering (SE)(Professional Elective – I) 4.gdoc', 'Python for Data Science (PDS) (Open Elective - I) 3.gdoc', 'scam7.csv', 'scam7.gsheet', 'scamss.pth', 'vectt.pkl', 'scamsss.pth', 'vecttt.pkl', 'scam1.pth', 'vect1.pkl', 'scams1.pth', 'vects.pkl', 'scam_detection_model.pth', 'tfidf_vectorizer.pkl', 'Untitled spreadsheet.gsheet', 'best_scam_detection_model.pth', 'best_sms_scam_detection_model.pth', 'sms_tfidf_vectorizer.pkl', 'sgmail.gsheet', 'sgmail.csv', 'gmail_tfidf_vectorizer.pkl', 'sgmails.csv', 'Copy of sgmails.csv', 'SGMAILS.csv', 'Copy of sgmails (1).gsheet', 'Copy of sgmails.gsheet', 'SGMAILSs.gsheet', 'SGMAILSscsv.csv', 'best_improved_gmail_scam_detection_model.pth', 'best_gmail_scam_detection_model.pth', 'gmail_hash_vectorizer.pkl', 'Untitled document.gdoc', 'verified_online.csv', 'verified_online.gsheet', 'dataset_phishing (2).gsheet', '5k.csv', 'phishing.csv', 'dataset_phishing (1).gsheet', 'dataset_phishing.gsheet', 'sweb.pkl', 'sweb.pth', 'sweeb11.pth', 'sweb12.pkl', 'aisc.csv', 'aisccc.csv', 'aicc.csv', 'aisc.gsheet', 'aisccc.gsheet', 'ais.csv', 'aics.csv', 'scam4.gsheet', 'aiscl.csv', 'aiscll.csv', 'smaj']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/dataset_phishing.csv'\n",
        "df = pd.read_csv(data_path, encoding='utf-8')\n",
        "# Load and preprocess data\n",
        "# Load and preprocess data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Features and target\n",
        "# Exclude 'url' from features as it's non-numerical\n",
        "features = [\n",
        "   'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn',\n",
        "    'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
        "    'ratio_digits_host', 'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain', 'nb_subdomains',\n",
        "    'prefix_suffix', 'random_domain', 'shortening_service', 'path_extension', 'nb_redirection', 'nb_external_redirection',\n",
        "    'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path',\n",
        "    'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path',\n",
        "    'phish_hints', 'domain_in_brand', 'brand_in_subdomain', 'brand_in_path', 'suspecious_tld',\n",
        "    'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS',\n",
        "    'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors', 'login_form',\n",
        "    'external_favicon', 'links_in_tags', 'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe',\n",
        "    'popup_window', 'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title',\n",
        "    'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length', 'domain_age'\n",
        "]\n",
        "\n",
        "\n",
        "df['status'] = df['status'].map({'phishing': 1, 'legitimate': 0})\n",
        "\n",
        "X = df[features] # Exclude the 'url' column\n",
        "y = df['status']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define the RNN model\n",
        "class PhishingRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
        "        super(PhishingRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_tensor.shape[1]\n",
        "hidden_size = 64\n",
        "output_size = 2\n",
        "num_layers = 2\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = PhishingRNN(input_size, hidden_size, output_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train_tensor.unsqueeze(1))\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluating the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(X_train_tensor.unsqueeze(1)).argmax(dim=1)\n",
        "    y_pred_test = model(X_test_tensor.unsqueeze(1)).argmax(dim=1)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train.numpy())\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test.numpy())\n",
        "\n",
        "    print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test.numpy()))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test.numpy()))\n",
        "\n",
        "# Function to test a new URL\n",
        "# Function to test a new URL\n",
        "def test_new_url(url_features):\n",
        "    # Ensure url_features has the correct number of features (87 in this case)\n",
        "    assert len(url_features) == len(features), f\"Expected {len(features)} features, but got {len(url_features)}.\"\n",
        "\n",
        "    model.eval()\n",
        "    url_features_scaled = scaler.transform([url_features])  # Scale the features\n",
        "    url_tensor = torch.tensor(url_features_scaled, dtype=torch.float32).unsqueeze(1)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(url_tensor).argmax(dim=1).item()\n",
        "    return 'Phishing' if prediction == 1 else 'Legitimate'\n",
        "\n",
        "new_url_features = [\n",
        "  23, 15, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 1, False, True, 0.0, False, False, False, False, False, -1, False, True, 0, 0.0, 0, 1, 0, 0.0, 14, 0.5714285714285714, 0.42857142857142855, 0.0, 0, 0, 0, 0, 0, 27, 0, 5, 1, 0, 0, 1, 0, 0, 0, 0, 0, 15, 6, 3, 0, 7, 0, 3, 7, 4.333333333333333, 0, 4.333333333333333, 0, 1, 0, 0, 0, 0, 0, 0, 1, 7305, 7119\n",
        "]\n",
        "# Example function to classify a new URL\n",
        "def test_new_url(url_features):\n",
        "    model.eval()\n",
        "    url_features_scaled = scaler.transform([url_features])  # Scale the features\n",
        "    url_tensor = torch.tensor(url_features_scaled, dtype=torch.float32).unsqueeze(1)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(url_tensor).argmax(dim=1).item()\n",
        "    return 'Phishing' if prediction == 1 else 'Legitimate'\n",
        "\n",
        "# Test the new URL\n",
        "result = test_new_url(new_url_features)\n",
        "print(f'The new URL is classified as: {result}')"
      ],
      "metadata": {
        "id": "rPMs5q6Z6DQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab116cc-3a70-4c19-bb4b-13254a8b3a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch [10/100], Loss: 0.3151\n",
            "Epoch [20/100], Loss: 0.2445\n",
            "Epoch [30/100], Loss: 0.2258\n",
            "Epoch [40/100], Loss: 0.2103\n",
            "Epoch [50/100], Loss: 0.1932\n",
            "Epoch [60/100], Loss: 0.1725\n",
            "Epoch [70/100], Loss: 0.1515\n",
            "Epoch [80/100], Loss: 0.1318\n",
            "Epoch [90/100], Loss: 0.1124\n",
            "Epoch [100/100], Loss: 0.0962\n",
            "Train Accuracy: 0.9662\n",
            "Test Accuracy: 0.9377\n",
            "Confusion Matrix:\n",
            " [[1346   76]\n",
            " [ 102 1334]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94      1422\n",
            "           1       0.95      0.93      0.94      1436\n",
            "\n",
            "    accuracy                           0.94      2858\n",
            "   macro avg       0.94      0.94      0.94      2858\n",
            "weighted avg       0.94      0.94      0.94      2858\n",
            "\n",
            "The new URL is classified as: Phishing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 tldextract python-whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEHxfVmx2yc8",
        "outputId": "81f3e7c1-0631-4f73-8a9d-c319bde42e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.15.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_whois-0.9.4-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, python-whois, tldextract\n",
            "Successfully installed python-whois-0.9.4 requests-file-2.1.0 tldextract-5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "import tldextract\n",
        "import whois\n",
        "import datetime\n",
        "import numpy as np\n",
        "import socket\n",
        "\n",
        "# Feature extraction function\n",
        "def is_valid_ip(ip):\n",
        "    \"\"\"Check if the hostname is an IP address.\"\"\"\n",
        "    try:\n",
        "        socket.inet_aton(ip)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Basic URL parsing\n",
        "    parsed_url = urlparse(url)\n",
        "    domain_info = tldextract.extract(url)\n",
        "\n",
        "    # Basic URL Length Features\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.hostname or '')\n",
        "\n",
        "    # Token Counts\n",
        "    tokens = ['.', '-', '@', '?', '&', '|', '=', '_', '~', '%', '/', '*', ':', ',', ';', '$', ' ']\n",
        "    token_names = ['nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore',\n",
        "                   'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma',\n",
        "                   'nb_semicolumn', 'nb_dollar', 'nb_space']\n",
        "\n",
        "    for token, name in zip(tokens, token_names):\n",
        "        features[name] = url.count(token)\n",
        "\n",
        "    features['nb_www'] = url.count('www')\n",
        "    features['nb_com'] = url.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    # Path Features\n",
        "    features['http_in_path'] = int('http' in parsed_url.path)\n",
        "    features['https_token'] = int('https' in url)\n",
        "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0\n",
        "\n",
        "    path_extensions = ['.php', '.asp', '.aspx', '.jsp']\n",
        "    features['path_extension'] = int(any(ext in parsed_url.path for ext in path_extensions))\n",
        "    features['prefix_suffix'] = int(any(part in url for part in ['.htm', '.html']))\n",
        "    random_strings = ['random', '1234', 'xyz']\n",
        "    features['random_domain'] = int(any(sub in parsed_url.hostname for sub in random_strings)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Domain Features\n",
        "    features['punycode'] = int(parsed_url.hostname.encode('idna').decode('utf-8') != parsed_url.hostname) if parsed_url.hostname else 0\n",
        "    shortening_services = ['bit.ly', 'tinyurl.com', 'goo.gl']\n",
        "    features['shortening_service'] = int(any(service in url for service in shortening_services))\n",
        "    features['port'] = parsed_url.port if parsed_url.port else -1\n",
        "    tlds = ['.com', '.net', '.org']\n",
        "    features['tld_in_path'] = int(any(tld in parsed_url.path for tld in tlds))\n",
        "    features['tld_in_subdomain'] = int(domain_info.suffix in parsed_url.hostname) if parsed_url.hostname else 0\n",
        "\n",
        "    # Check if IP address is used as hostname\n",
        "    features['ip'] = int(is_valid_ip(parsed_url.hostname)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Ratio of digits in hostname\n",
        "    features['ratio_digits_host'] = sum(c.isdigit() for c in parsed_url.hostname) / len(parsed_url.hostname) if parsed_url.hostname and len(parsed_url.hostname) > 0 else 0\n",
        "\n",
        "    # Abnormal subdomain\n",
        "    features['abnormal_subdomain'] = int(len(domain_info.subdomain.split('.')) > 2) if domain_info.subdomain else 0\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['nb_subdomains'] = len(domain_info.subdomain.split('.')) if domain_info.subdomain else 0\n",
        "\n",
        "    # Initialize media ratios\n",
        "    features['ratio_intMedia'] = 0\n",
        "    features['ratio_extMedia'] = 0\n",
        "\n",
        "    # Default values for URL content-based features\n",
        "    default_features = {\n",
        "        'nb_hyperlinks': 0,\n",
        "        'ratio_extHyperlinks': 0,\n",
        "        'ratio_intHyperlinks': 0,\n",
        "        'ratio_nullHyperlinks': 0,\n",
        "        'nb_redirection': 0,\n",
        "        'nb_external_redirection': 0,\n",
        "        'ratio_intRedirection': 0,\n",
        "        'ratio_extRedirection': 0,\n",
        "        'links_in_tags': 0,\n",
        "        'external_favicon': 0,\n",
        "        'iframe': 0,\n",
        "        'popup_window': 0,\n",
        "        'sfh': 0,\n",
        "        'safe_anchor': 0,\n",
        "        'onmouseover': 0,\n",
        "        'right_clic': 0,\n",
        "        'empty_title': 0,\n",
        "        'domain_in_title': 0,\n",
        "        'domain_with_copyright': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Count Links in the Page\n",
        "        links = soup.find_all('a')\n",
        "        total_links = len(links)\n",
        "        features['nb_hyperlinks'] = total_links\n",
        "\n",
        "        if total_links > 0:\n",
        "            ext_links = len([link for link in links if link.get('href', '').startswith('http')])\n",
        "            int_links = len([link for link in links if not link.get('href', '').startswith('http')])\n",
        "            null_links = len([link for link in links if not link.get('href')])\n",
        "\n",
        "            features['ratio_extHyperlinks'] = ext_links / total_links\n",
        "            features['ratio_intHyperlinks'] = int_links / total_links\n",
        "            features['ratio_nullHyperlinks'] = null_links / total_links\n",
        "        else:\n",
        "            features['ratio_extHyperlinks'] = 0\n",
        "            features['ratio_intHyperlinks'] = 0\n",
        "            features['ratio_nullHyperlinks'] = 0\n",
        "\n",
        "        # Calculate redirections\n",
        "        features['nb_redirection'] = len(response.history)\n",
        "        features['nb_external_redirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc != parsed_url.netloc])\n",
        "\n",
        "        # Internal and external redirection ratios\n",
        "        features['ratio_intRedirection'] = len([resp for resp in response.history if urlparse(resp.url).netloc == parsed_url.netloc]) / len(response.history) if response.history else 0\n",
        "        features['ratio_extRedirection'] = features['nb_external_redirection'] / len(response.history) if response.history else 0\n",
        "\n",
        "        # Check for favicon\n",
        "        favicon_tag = soup.find('link', rel='shortcut icon')\n",
        "        features['external_favicon'] = int(favicon_tag and parsed_url.netloc not in favicon_tag.get('href', ''))\n",
        "\n",
        "        # Links in specific tags\n",
        "        script_links = len(soup.find_all('script', src=True))\n",
        "        link_links = len(soup.find_all('link', href=True))\n",
        "        features['links_in_tags'] = script_links + link_links\n",
        "\n",
        "        media_links = soup.find_all(['img', 'video', 'audio'])\n",
        "        features['ratio_extMedia'] = len(media_links) / len(links) if len(links) > 0 else 0\n",
        "        features['iframe'] = int(bool(soup.find('iframe')))\n",
        "        features['popup_window'] = int(bool(soup.find('script', string=re.compile('window.open'))))\n",
        "\n",
        "        safe_anchors = [link.get('rel') != 'nofollow' for link in links]\n",
        "        features['sfh'] = int(any(form.get('action', '').startswith('http') for form in soup.find_all('form')))\n",
        "        features['safe_anchor'] = int(all(safe_anchors))\n",
        "\n",
        "        features['onmouseover'] = int(bool(soup.find_all(attrs={\"onmouseover\": True})))\n",
        "        features['right_clic'] = int(bool(soup.find_all(attrs={\"oncontextmenu\": True})))\n",
        "        features['empty_title'] = int(bool(soup.find('title') and not soup.find('title').text.strip()))\n",
        "        features['domain_in_title'] = int(domain_info.domain in (soup.find('title').text if soup.find('title') else ''))\n",
        "        features['domain_with_copyright'] = int('copyright' in (soup.find('body').text if soup.find('body') else '').lower())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        features.update(default_features)\n",
        "\n",
        "    # Calculate the lengths of words\n",
        "    path_words = re.findall(r'\\w+', parsed_url.path)\n",
        "    host_words = re.findall(r'\\w+', parsed_url.hostname) if parsed_url.hostname else []\n",
        "    all_words = path_words + host_words\n",
        "    features['length_words_raw'] = len(' '.join(all_words))\n",
        "\n",
        "    features['char_repeat'] = len({char: url.count(char) for char in set(url) if url.count(char) > 1})\n",
        "\n",
        "    # Capture lengths of words\n",
        "    features['shortest_word_host'] = len(min(host_words, key=len, default=''))\n",
        "    features['shortest_word_path'] = len(min(path_words, key=len, default=''))\n",
        "    features['longest_word_host'] = len(max(host_words, key=len, default=''))\n",
        "    features['longest_word_path'] = len(max(path_words, key=len, default=''))\n",
        "\n",
        "    features['shortest_words_raw'] = len(min(all_words, key=len, default=''))\n",
        "    features['longest_words_raw'] = len(max(all_words, key=len, default=''))\n",
        "    features['avg_word_host'] = np.mean([len(word) for word in host_words]) if host_words else 0\n",
        "    features['avg_word_path'] = np.mean([len(word) for word in path_words]) if path_words else 0\n",
        "    features['avg_words_raw'] = np.mean([len(word) for word in all_words]) if all_words else 0\n",
        "\n",
        "    features['phish_hints'] = int(any(hint in url for hint in ['secure', 'login', 'update', 'verify']))\n",
        "\n",
        "    features['domain_in_brand'] = int(domain_info.domain in url)\n",
        "    features['brand_in_subdomain'] = int(any(brand in parsed_url.hostname for brand in ['brand', 'company'])) if parsed_url.hostname else 0\n",
        "    features['brand_in_path'] = int(any(brand in parsed_url.path for brand in ['brand', 'company']))\n",
        "\n",
        "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq']\n",
        "    features['suspecious_tld'] = int(any(tld in parsed_url.hostname for tld in suspicious_tlds)) if parsed_url.hostname else 0\n",
        "\n",
        "    # Placeholder for Int Errors and Ext Errors\n",
        "    features['ratio_intErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['ratio_extErrors'] = 0  # Placeholder, logic needed to calculate\n",
        "    features['submit_email'] = int(bool(soup.find('input', {'type': 'email'}))) if 'soup' in locals() else 0\n",
        "\n",
        "    try:\n",
        "        whois_info = whois.whois(parsed_url.hostname)\n",
        "        features['whois_registered_domain'] = int(bool(whois_info.domain_name))\n",
        "        if whois_info.creation_date and whois_info.expiration_date:\n",
        "            if isinstance(whois_info.creation_date, list):\n",
        "                creation_date = whois_info.creation_date[0]\n",
        "            else:\n",
        "                creation_date = whois_info.creation_date\n",
        "\n",
        "            if isinstance(whois_info.expiration_date, list):\n",
        "                expiration_date = whois_info.expiration_date[0]\n",
        "            else:\n",
        "                expiration_date = whois_info.expiration_date\n",
        "\n",
        "            features['domain_registration_length'] = (expiration_date - creation_date).days\n",
        "            features['domain_age'] = (datetime.datetime.now() - creation_date).days\n",
        "        else:\n",
        "            features['domain_registration_length'] = -1\n",
        "            features['domain_age'] = -1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching WHOIS info: {e}\")\n",
        "        features['whois_registered_domain'] = 0\n",
        "        features['domain_registration_length'] = -1\n",
        "        features['domain_age'] = -1\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example usage\n",
        "url = 'https://www.facebook.com'\n",
        "extracted_features = extract_features(url)\n",
        "\n",
        "# Print features in list format including the URL and feature names\n",
        "feature_names = ['URL'] + list(extracted_features.keys())\n",
        "feature_values = [url] + list(extracted_features.values())\n",
        "feature_list = list(zip(feature_names, feature_values))\n",
        "feature_lists = [url] + [value for value in extracted_features.values()]\n",
        "print(\"f\", feature_list)\n",
        "print(\"ff\", feature_lists)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNlIlQNF2pQD",
        "outputId": "4327077d-841f-44be-b410-2aac2f717075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching URL content: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "f [('URL', 'https://www.facebook.com'), ('length_url', 24), ('length_hostname', 16), ('nb_dots', 2), ('nb_hyphens', 0), ('nb_at', 0), ('nb_qm', 0), ('nb_and', 0), ('nb_or', 0), ('nb_eq', 0), ('nb_underscore', 0), ('nb_tilde', 0), ('nb_percent', 0), ('nb_slash', 2), ('nb_star', 0), ('nb_colon', 1), ('nb_comma', 0), ('nb_semicolumn', 0), ('nb_dollar', 0), ('nb_space', 0), ('nb_www', 1), ('nb_com', 1), ('nb_dslash', 1), ('http_in_path', 0), ('https_token', 1), ('ratio_digits_url', 0.0), ('path_extension', 0), ('prefix_suffix', 0), ('random_domain', 0), ('punycode', 0), ('shortening_service', 0), ('port', -1), ('tld_in_path', 0), ('tld_in_subdomain', 1), ('ip', 0), ('ratio_digits_host', 0.0), ('abnormal_subdomain', 0), ('nb_subdomains', 1), ('ratio_intMedia', 0), ('ratio_extMedia', 0), ('nb_hyperlinks', 0), ('ratio_extHyperlinks', 0), ('ratio_intHyperlinks', 0), ('ratio_nullHyperlinks', 0), ('nb_redirection', 0), ('nb_external_redirection', 0), ('ratio_intRedirection', 0), ('ratio_extRedirection', 0), ('links_in_tags', 0), ('external_favicon', 0), ('iframe', 0), ('popup_window', 0), ('sfh', 0), ('safe_anchor', 0), ('onmouseover', 0), ('right_clic', 0), ('empty_title', 0), ('domain_in_title', 0), ('domain_with_copyright', 0), ('length_words_raw', 16), ('char_repeat', 6), ('shortest_word_host', 3), ('shortest_word_path', 0), ('longest_word_host', 8), ('longest_word_path', 0), ('shortest_words_raw', 3), ('longest_words_raw', 8), ('avg_word_host', 4.666666666666667), ('avg_word_path', 0), ('avg_words_raw', 4.666666666666667), ('phish_hints', 0), ('domain_in_brand', 1), ('brand_in_subdomain', 0), ('brand_in_path', 0), ('suspecious_tld', 0), ('ratio_intErrors', 0), ('ratio_extErrors', 0), ('submit_email', 0), ('whois_registered_domain', 1), ('domain_registration_length', 13149), ('domain_age', 10001)]\n",
            "ff ['https://www.facebook.com', 24, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0.0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0.0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 6, 3, 0, 8, 0, 3, 8, 4.666666666666667, 0, 4.666666666666667, 0, 1, 0, 0, 0, 0, 0, 0, 1, 13149, 10001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# URL Feature Extraction\n",
        "def fetch_url_content(url):\n",
        "    \"\"\"Fetch URL content. Dummy implementation for illustration.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_features(url):\n",
        "    \"\"\"Extract features from URL for phishing detection.\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Basic Length-based Features\n",
        "    features['url_length'] = len(url)\n",
        "    features['hostname_length'] = len(urlparse(url).hostname or '')\n",
        "    features['path_length'] = len(urlparse(url).path or '')\n",
        "\n",
        "    # Token-based Features\n",
        "    features['num_tokens'] = len(url.split('/'))\n",
        "\n",
        "    # Presence of Special Characters\n",
        "    features['num_dots'] = url.count('.')\n",
        "    features['num_hyphens'] = url.count('-')\n",
        "    features['num_ip'] = int(bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url)))\n",
        "\n",
        "    # Fetch URL content and calculate content-based features\n",
        "    try:\n",
        "        content = fetch_url_content(url)\n",
        "        if content:\n",
        "            num_words = len(content.split())\n",
        "            features['num_words'] = num_words\n",
        "            features['longest_word'] = len(max(content.split(), key=len, default=''))\n",
        "        else:\n",
        "            features['num_words'] = 0\n",
        "            features['longest_word'] = 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URL content: {e}\")\n",
        "        features['num_words'] = 0\n",
        "        features['longest_word'] = 0\n",
        "\n",
        "    return features\n",
        "\n",
        "# Load and Prepare Data\n",
        "def load_and_prepare_data(filepath):\n",
        "    \"\"\"Load dataset and prepare for training.\"\"\"\n",
        "    data = pd.read_csv(filepath)\n",
        "    X = data['url'].apply(extract_features).tolist()\n",
        "    y = data['label']\n",
        "\n",
        "    # Convert features to a format suitable for machine learning\n",
        "    vec = DictVectorizer()\n",
        "    X_vectorized = vec.fit_transform(X)\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "# Define the Neural Network Model\n",
        "class PhishingModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(PhishingModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test = load_and_prepare_data('/content/drive/MyDrive/dataset_phishing.csv')\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled.toarray(), dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled.toarray(), dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    model = PhishingModel(input_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model\n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(X_train_tensor)\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct = (predicted == y_test_tensor).sum().item()\n",
        "        accuracy = correct / len(y_test_tensor)\n",
        "        print(f'Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "mUwzzMPFc9f-",
        "outputId": "d62977df-fe41-4f58-c50f-fae50579ffba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['label'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9ba309ebcd8b>\u001b[0m in \u001b[0;36m<cell line: 237>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;31m# Load and preprocess dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/dataset_phishing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9ba309ebcd8b>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5342\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5343\u001b[0m         \"\"\"\n\u001b[0;32m-> 5344\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5345\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5346\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4710\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4711\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4752\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4753\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4754\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7000\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7001\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['label'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pK7iw-l8nx-",
        "outputId": "155d79ea-6770-4172-ac8d-7dd9926c3830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from urllib.parse import urlparse\n",
        "import tldextract\n",
        "import re\n",
        "\n",
        "# Function to extract features from a URL\n",
        "def extract_features(url):\n",
        "    # Parse the URL\n",
        "    parsed_url = urlparse(url)\n",
        "    ext = tldextract.extract(url)\n",
        "\n",
        "    # Extract features\n",
        "    features = {\n",
        "        'having_IP': 1 if re.match(r\"(\\d{1,3}\\.){3}\\d{1,3}\", parsed_url.netloc) else 0,\n",
        "        'URL_Length': len(url),\n",
        "        'Shortining_Service': 1 if any(shortener in url for shortener in [\"bit.ly\", \"tinyurl.com\", \"goo.gl\", \"ow.ly\", \"t.co\"]) else 0,\n",
        "        'having_At_Symbol': 1 if '@' in url else 0,\n",
        "        'double_slash_redirecting': 1 if '//' in parsed_url.path else 0,\n",
        "        'Prefix_Suffix': 1 if '-' in ext.domain else 0,\n",
        "        'having_Sub_Domain': 1 if ext.subdomain else 0,\n",
        "        'SSLfinal_State': 1 if parsed_url.scheme == 'https' else 0,\n",
        "        'Domain_registeration_length': 0, # Placeholder, you need to implement this feature\n",
        "        'Favicon': 0, # Placeholder, you need to implement this feature\n",
        "        'port': parsed_url.port if parsed_url.port else 0,\n",
        "        'HTTPS_token': 1 if 'https' in parsed_url.netloc.lower() else 0,\n",
        "        'Request_URL': 0, # Placeholder, you need to implement this feature\n",
        "        'URL_of_Anchor': 0, # Placeholder, you need to implement this feature\n",
        "        'Links_in_tags': 0, # Placeholder, you need to implement this feature\n",
        "        'SFH': 0, # Placeholder, you need to implement this feature\n",
        "        'Submitting_to_email': 0, # Placeholder, you need to implement this feature\n",
        "        'Abnormal_URL': 0, # Placeholder, you need to implement this feature\n",
        "        'Redirect': 0, # Placeholder, you need to implement this feature\n",
        "        'on_mouseover': 0, # Placeholder, you need to implement this feature\n",
        "        'RightClick': 0, # Placeholder, you need to implement this feature\n",
        "        'popUpWidnow': 0, # Placeholder, you need to implement this feature\n",
        "        'Iframe': 0, # Placeholder, you need to implement this feature\n",
        "        'age_of_domain': 0, # Placeholder, you need to implement this feature\n",
        "        'DNSRecord': 0, # Placeholder, you need to implement this feature\n",
        "        'web_traffic': 0, # Placeholder, you need to implement this feature\n",
        "        'Page_Rank': 0, # Placeholder, you need to implement this feature\n",
        "        'Google_Index': 0, # Placeholder, you need to implement this feature\n",
        "        'Links_pointing_to_page': 0, # Placeholder, you need to implement this feature\n",
        "        'Statistical_report': 0, # Placeholder, you need to implement this feature\n",
        "        'domain_length': len(ext.domain),\n",
        "        'subdomain_length': len(ext.subdomain),\n",
        "        'suffix_length': len(ext.suffix),\n",
        "        'path_length': len(parsed_url.path),\n",
        "        'query_length': len(parsed_url.query),\n",
        "        'has_https': 1 if parsed_url.scheme == 'https' else 0,\n",
        "        'num_digits_in_domain': len(re.findall(r'\\d', ext.domain)),\n",
        "        'num_subdomains': ext.subdomain.count('.') + 1 if ext.subdomain else 0,\n",
        "        'num_special_chars': len(re.findall(r'\\W', url)),\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame([features]) # Return a DataFrame with all 30 features\n",
        "\n",
        "# Load dataset and prepare the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/phishing.csv')\n",
        "\n",
        "# Extract target\n",
        "X = data.copy()\n",
        "y = X.pop('Target')\n",
        "\n",
        "# Drop unwanted columns\n",
        "X = data.drop(columns=['index','Target'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, train_size=0.75)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
        "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Create DataLoader for batches\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Define the PyTorch model\n",
        "class PhishingDetectionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(PhishingDetectionModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = PhishingDetectionModel(X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "    return running_loss / len(train_loader.dataset)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in valid_loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "            preds = outputs.round()\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(valid_loader.dataset), accuracy\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 200\n",
        "early_stopping_patience = 20\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy = validate(model, valid_loader, criterion)\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_val_loss - 0.01:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Function to predict if a URL is suspicious\n",
        "def predict_url(url):\n",
        "    features = extract_features(url)\n",
        "    features = scaler.transform(features)  # Scale the features\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(features_tensor)\n",
        "        return \"Suspicious\" if prediction.item() >= 0.5 else \"Not Suspicious\"\n",
        "\n",
        "# Example usage: Predicting a URL\n",
        "url_to_test = \"http://example.com\"\n",
        "result = predict_url(url_to_test)\n",
        "print(f\"The URL {url_to_test} is {result}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZeP_yM2C7xi0",
        "outputId": "3328d859-1120-4f2b-a6db-5d1885615320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Train Loss: 0.2485, Val Loss: 0.1801, Val Accuracy: 0.9338\n",
            "Epoch 2/200, Train Loss: 0.1649, Val Loss: 0.1487, Val Accuracy: 0.9399\n",
            "Epoch 3/200, Train Loss: 0.1472, Val Loss: 0.1385, Val Accuracy: 0.9468\n",
            "Epoch 4/200, Train Loss: 0.1354, Val Loss: 0.1256, Val Accuracy: 0.9472\n",
            "Epoch 5/200, Train Loss: 0.1251, Val Loss: 0.1148, Val Accuracy: 0.9555\n",
            "Epoch 6/200, Train Loss: 0.1172, Val Loss: 0.1153, Val Accuracy: 0.9533\n",
            "Epoch 7/200, Train Loss: 0.1064, Val Loss: 0.1048, Val Accuracy: 0.9551\n",
            "Epoch 8/200, Train Loss: 0.1036, Val Loss: 0.1022, Val Accuracy: 0.9573\n",
            "Epoch 9/200, Train Loss: 0.1036, Val Loss: 0.0999, Val Accuracy: 0.9602\n",
            "Epoch 10/200, Train Loss: 0.0970, Val Loss: 0.0960, Val Accuracy: 0.9606\n",
            "Epoch 11/200, Train Loss: 0.0917, Val Loss: 0.0979, Val Accuracy: 0.9569\n",
            "Epoch 12/200, Train Loss: 0.0879, Val Loss: 0.0914, Val Accuracy: 0.9635\n",
            "Epoch 13/200, Train Loss: 0.0900, Val Loss: 0.0924, Val Accuracy: 0.9627\n",
            "Epoch 14/200, Train Loss: 0.0859, Val Loss: 0.0889, Val Accuracy: 0.9609\n",
            "Epoch 15/200, Train Loss: 0.0804, Val Loss: 0.0839, Val Accuracy: 0.9656\n",
            "Epoch 16/200, Train Loss: 0.0754, Val Loss: 0.0831, Val Accuracy: 0.9660\n",
            "Epoch 17/200, Train Loss: 0.0709, Val Loss: 0.0873, Val Accuracy: 0.9642\n",
            "Epoch 18/200, Train Loss: 0.0710, Val Loss: 0.0839, Val Accuracy: 0.9664\n",
            "Epoch 19/200, Train Loss: 0.0717, Val Loss: 0.0795, Val Accuracy: 0.9664\n",
            "Epoch 20/200, Train Loss: 0.0681, Val Loss: 0.0777, Val Accuracy: 0.9649\n",
            "Epoch 21/200, Train Loss: 0.0684, Val Loss: 0.0781, Val Accuracy: 0.9664\n",
            "Epoch 22/200, Train Loss: 0.0629, Val Loss: 0.0800, Val Accuracy: 0.9660\n",
            "Epoch 23/200, Train Loss: 0.0629, Val Loss: 0.0759, Val Accuracy: 0.9664\n",
            "Epoch 24/200, Train Loss: 0.0593, Val Loss: 0.0745, Val Accuracy: 0.9671\n",
            "Epoch 25/200, Train Loss: 0.0604, Val Loss: 0.0736, Val Accuracy: 0.9725\n",
            "Epoch 26/200, Train Loss: 0.0612, Val Loss: 0.0750, Val Accuracy: 0.9685\n",
            "Epoch 27/200, Train Loss: 0.0634, Val Loss: 0.0834, Val Accuracy: 0.9653\n",
            "Epoch 28/200, Train Loss: 0.0639, Val Loss: 0.0767, Val Accuracy: 0.9692\n",
            "Epoch 29/200, Train Loss: 0.0533, Val Loss: 0.0769, Val Accuracy: 0.9703\n",
            "Epoch 30/200, Train Loss: 0.0533, Val Loss: 0.0702, Val Accuracy: 0.9711\n",
            "Epoch 31/200, Train Loss: 0.0550, Val Loss: 0.0720, Val Accuracy: 0.9692\n",
            "Epoch 32/200, Train Loss: 0.0516, Val Loss: 0.0735, Val Accuracy: 0.9692\n",
            "Epoch 33/200, Train Loss: 0.0506, Val Loss: 0.0739, Val Accuracy: 0.9689\n",
            "Epoch 34/200, Train Loss: 0.0490, Val Loss: 0.0714, Val Accuracy: 0.9696\n",
            "Epoch 35/200, Train Loss: 0.0523, Val Loss: 0.0702, Val Accuracy: 0.9718\n",
            "Epoch 36/200, Train Loss: 0.0540, Val Loss: 0.0737, Val Accuracy: 0.9696\n",
            "Epoch 37/200, Train Loss: 0.0484, Val Loss: 0.0704, Val Accuracy: 0.9725\n",
            "Epoch 38/200, Train Loss: 0.0490, Val Loss: 0.0697, Val Accuracy: 0.9721\n",
            "Epoch 39/200, Train Loss: 0.0487, Val Loss: 0.0721, Val Accuracy: 0.9682\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- URL_Length\n- domain_length\n- has_https\n- having_IP\n- num_digits_in_domain\n- ...\nFeature names seen at fit time, yet now missing:\n- URLURL_Length\n- having_IPhaving_IP_Address\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ca908cdc9513>\u001b[0m in \u001b[0;36m<cell line: 187>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m# Example usage: Predicting a URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0murl_to_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://example.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The URL {url_to_test} is {result}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ca908cdc9513>\u001b[0m in \u001b[0;36mpredict_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Scale the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mfeatures_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \"\"\"\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 )\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     def _validate_data(\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- URL_Length\n- domain_length\n- has_https\n- having_IP\n- num_digits_in_domain\n- ...\nFeature names seen at fit time, yet now missing:\n- URLURL_Length\n- having_IPhaving_IP_Address\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Use the output from the last time step\n",
        "        return out\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/phishing.csv')\n",
        "\n",
        "# Convert categorical features to numerical values\n",
        "label_encoders = {}\n",
        "for column in ['UsingIP', 'LongURL', 'ShortURL', 'Symbol@', 'Redirecting//', 'PrefixSuffix-', 'SubDomains', 'HTTPS', 'Favicon', 'NonStdPort', 'HTTPSDomainURL', 'RequestURL', 'AnchorURL', 'LinksInScriptTags', 'ServerFormHandler', 'InfoEmail', 'AbnormalURL', 'WebsiteForwarding', 'StatusBarCust', 'DisableRightClick', 'UsingPopupWindow', 'IframeRedirection']:\n",
        "    le = LabelEncoder()\n",
        "    data[column] = le.fit_transform(data[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Define features and target variable\n",
        "features = ['UsingIP', 'LongURL', 'ShortURL', 'Symbol@', 'Redirecting//', 'PrefixSuffix-', 'SubDomains', 'HTTPS', 'DomainRegLen', 'Favicon', 'NonStdPort', 'HTTPSDomainURL', 'RequestURL', 'AnchorURL', 'LinksInScriptTags', 'ServerFormHandler', 'InfoEmail', 'AbnormalURL', 'WebsiteForwarding', 'StatusBarCust', 'DisableRightClick', 'UsingPopupWindow', 'IframeRedirection', 'AgeofDomain', 'DNSRecording', 'WebsiteTraffic', 'PageRank', 'GoogleIndex', 'LinksPointingToPage', 'StatsReport']\n",
        "X = data[features].values\n",
        "y = data['class'].values  # Assuming 'class' is your target variable\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "class URLDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = URLDataset(X_train, y_train)\n",
        "test_dataset = URLDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the RNN model\n",
        "input_size = len(features)\n",
        "hidden_size = 128\n",
        "output_size = len(np.unique(y))\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features.unsqueeze(1))  # Add batch dimension\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            outputs = model(features.unsqueeze(1))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'rnn_model.pth')\n",
        "\n",
        "# Load the model for prediction\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "model.load_state_dict(torch.load('rnn_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Load sample data from CSV\n",
        "sample_data = pd.read_csv('/content/drive/MyDrive/phishing.csv')\n",
        "\n",
        "# Convert categorical features to numerical values using the same encoders\n",
        "for column, le in label_encoders.items():\n",
        "    sample_data[column] = le.transform(sample_data[column])\n",
        "\n",
        "# Normalize sample data using the same scaler\n",
        "X_sample = scaler.transform(sample_data[features])\n",
        "\n",
        "# Convert sample data to PyTorch tensor\n",
        "X_sample_tensor = torch.tensor(X_sample, dtype=torch.float32).unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_sample_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print(f'Predicted classes: {predicted.numpy()}')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0vUlNPAk_FvK",
        "outputId": "6264b23f-9096-4efa-d8fa-d5e486f33656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Target -1 is out of bounds.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-414757d0b999>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1186\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target -1 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/phishing.csv\")\n",
        "data = data.drop(['Index'], axis=1)\n",
        "\n",
        "# Splitting the dataset into dependent and independent features\n",
        "X = data.drop([\"class\"], axis=1)\n",
        "y = data[\"class\"]\n",
        "\n",
        "y = (y == 1).astype(int)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the Fully Connected Neural Network model\n",
        "class PhishingFCNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PhishingFCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Model instantiation\n",
        "input_size = X_train.shape[1]  # This should be the number of features\n",
        "hidden_size = 128\n",
        "output_size = 1\n",
        "\n",
        "model = PhishingFCNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 20\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        labels = labels.unsqueeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test).round().squeeze()\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_test = y_test.numpy()\n",
        "y_pred = y_pred.numpy()\n",
        "\n",
        "# Metrics calculation\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {acc:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n",
        "print(f'Recall: {rec:.3f}')\n",
        "print(f'Precision: {prec:.3f}')\n",
        "print(f'Classification Report:\\n{report}')\n",
        "\n",
        "# Sample input for testing\n",
        "# Make sure the number of features matches the input_size of the model\n",
        "sample_input = np.array([[1, 0, 0, 1, 1, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]])  # Adjust the size here\n",
        "sample_input_tensor = torch.tensor(sample_input, dtype=torch.float32)\n",
        "\n",
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_prediction = model(sample_input_tensor).item()\n",
        "    predicted_class = 1 if sample_prediction >= 0.5 else 0\n",
        "\n",
        "print(f'Sample input prediction: {predicted_class} (Probability: {sample_prediction:.4f})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlrnToeB3QtC",
        "outputId": "1d35c0f5-372c-4592-ba13-c96b834b8086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2912\n",
            "Epoch [2/20], Loss: 0.0463\n",
            "Epoch [3/20], Loss: 0.2567\n",
            "Epoch [4/20], Loss: 0.3615\n",
            "Epoch [5/20], Loss: 0.0453\n",
            "Epoch [6/20], Loss: 0.3764\n",
            "Epoch [7/20], Loss: 0.1466\n",
            "Epoch [8/20], Loss: 0.0597\n",
            "Epoch [9/20], Loss: 0.2109\n",
            "Epoch [10/20], Loss: 0.0437\n",
            "Epoch [11/20], Loss: 0.2620\n",
            "Epoch [12/20], Loss: 0.0206\n",
            "Epoch [13/20], Loss: 0.0925\n",
            "Epoch [14/20], Loss: 0.1682\n",
            "Epoch [15/20], Loss: 0.0809\n",
            "Epoch [16/20], Loss: 0.0433\n",
            "Epoch [17/20], Loss: 0.0300\n",
            "Epoch [18/20], Loss: 0.0673\n",
            "Epoch [19/20], Loss: 0.0601\n",
            "Epoch [20/20], Loss: 0.1412\n",
            "Accuracy: 0.956\n",
            "F1 Score: 0.961\n",
            "Recall: 0.972\n",
            "Precision: 0.951\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.94      0.95       976\n",
            "         1.0       0.95      0.97      0.96      1235\n",
            "\n",
            "    accuracy                           0.96      2211\n",
            "   macro avg       0.96      0.95      0.96      2211\n",
            "weighted avg       0.96      0.96      0.96      2211\n",
            "\n",
            "Sample input prediction: 1 (Probability: 0.9965)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU79-lIp6LeO",
        "outputId": "6599f565-59c6-46ec-ae3d-3e7c949a95de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: whois in /usr/local/lib/python3.10/dist-packages (1.20240129.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipaddress\n",
        "import re\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import socket\n",
        "import requests\n",
        "from googlesearch import search\n",
        "import whois\n",
        "from datetime import date, datetime\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class FeatureExtraction:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.features = []\n",
        "        self.domain = \"\"\n",
        "        self.whois_response = \"\"\n",
        "        self.urlparse = \"\"\n",
        "        self.response = \"\"\n",
        "        self.soup = \"\"\n",
        "\n",
        "        try:\n",
        "            self.response = requests.get(url)\n",
        "            self.soup = BeautifulSoup(self.response.text, 'html.parser')\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching URL: {e}\")\n",
        "\n",
        "        try:\n",
        "            self.urlparse = urlparse(url)\n",
        "            self.domain = self.urlparse.netloc\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing URL: {e}\")\n",
        "\n",
        "        try:\n",
        "            self.whois_response = whois.whois(self.domain)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching WHOIS data: {e}\")\n",
        "\n",
        "        # Extracting features\n",
        "        self.features.append(self.using_ip())\n",
        "        self.features.append(self.long_url())\n",
        "        self.features.append(self.short_url())\n",
        "        self.features.append(self.symbol_in_url())\n",
        "        self.features.append(self.redirecting())\n",
        "        self.features.append(self.prefix_suffix())\n",
        "        self.features.append(self.sub_domains())\n",
        "        self.features.append(self.https())\n",
        "        self.features.append(self.domain_registration_length())\n",
        "        self.features.append(self.favicon())\n",
        "\n",
        "        # Add more feature extraction methods as necessary\n",
        "        # ...\n",
        "\n",
        "    def using_ip(self):\n",
        "        try:\n",
        "            ipaddress.ip_address(self.url)\n",
        "            return -1\n",
        "        except ValueError:\n",
        "            return 1\n",
        "\n",
        "    def long_url(self):\n",
        "        if len(self.url) < 54:\n",
        "            return 1\n",
        "        elif len(self.url) >= 54 and len(self.url) <= 75:\n",
        "            return 0\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def short_url(self):\n",
        "        if re.search(r'bit\\.ly|goo\\.gl|shorte\\.st|...|tinyurl\\.com', self.url):\n",
        "            return -1\n",
        "        return 1\n",
        "\n",
        "    def symbol_in_url(self):\n",
        "        if \"@\" in self.url:\n",
        "            return -1\n",
        "        return 1\n",
        "\n",
        "    def redirecting(self):\n",
        "        if self.url.rfind('//') > 6:\n",
        "            return -1\n",
        "        return 1\n",
        "\n",
        "    def prefix_suffix(self):\n",
        "        if '-' in self.domain:\n",
        "            return -1\n",
        "        return 1\n",
        "\n",
        "    def sub_domains(self):\n",
        "        dot_count = len(re.findall(\"\\.\", self.url))\n",
        "        if dot_count == 1:\n",
        "            return 1\n",
        "        elif dot_count == 2:\n",
        "            return 0\n",
        "        return -1\n",
        "\n",
        "    def https(self):\n",
        "        if self.urlparse.scheme == 'https':\n",
        "            return 1\n",
        "        return -1\n",
        "\n",
        "    def domain_registration_length(self):\n",
        "        try:\n",
        "            expiration_date = self.whois_response.expiration_date\n",
        "            creation_date = self.whois_response.creation_date\n",
        "            if isinstance(expiration_date, list):\n",
        "                expiration_date = expiration_date[0]\n",
        "            if isinstance(creation_date, list):\n",
        "                creation_date = creation_date[0]\n",
        "\n",
        "            age = (expiration_date.year - creation_date.year) * 12 + (expiration_date.month - creation_date.month)\n",
        "            if age >= 12:\n",
        "                return 1\n",
        "            return -1\n",
        "        except Exception:\n",
        "            return -1\n",
        "\n",
        "    def favicon(self):\n",
        "        try:\n",
        "            for head in self.soup.find_all('head'):\n",
        "                for link in head.find_all('link', href=True):\n",
        "                    if self.domain in link['href']:\n",
        "                        return 1\n",
        "            return -1\n",
        "        except Exception:\n",
        "            return -1\n",
        "\n",
        "    # Add more methods for other features...\n",
        "\n",
        "# Example usage:\n",
        "url = \"http://example.com\"\n",
        "fe = FeatureExtraction(url)\n",
        "print(fe.features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PImCfDg6AWt",
        "outputId": "6b182779-a04f-4ffc-cf6c-b0351386a2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching WHOIS data: module 'whois' has no attribute 'whois'\n",
            "[1, 1, -1, 1, 1, 1, 1, -1, -1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whois\n",
        "import datetime\n",
        "from urllib.parse import urlparse\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_whois_info(url):\n",
        "    try:\n",
        "        # Extract domain from URL\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc or parsed_url.path\n",
        "        if not domain:\n",
        "            raise ValueError(\"Invalid URL format\")\n",
        "\n",
        "        logger.debug(f\"Extracted domain: {domain}\")\n",
        "\n",
        "        # Fetch WHOIS information using whois library\n",
        "        logger.debug(\"Starting WHOIS lookup\")\n",
        "        w = whois.whois(domain)\n",
        "        logger.debug(\"WHOIS lookup completed\")\n",
        "\n",
        "        # Extract domain name, creation date, and expiration date\n",
        "        domain_name = w.domain_name\n",
        "        creation_date = w.creation_date\n",
        "        expiration_date = w.expiration_date\n",
        "\n",
        "        logger.debug(f\"WHOIS data - Domain Name: {domain_name}, Creation Date: {creation_date}, Expiration Date: {expiration_date}\")\n",
        "\n",
        "        # Handle lists and calculate registration length and domain age\n",
        "        if isinstance(creation_date, list):\n",
        "            creation_date = creation_date[0]\n",
        "        if isinstance(expiration_date, list):\n",
        "            expiration_date = expiration_date[0]\n",
        "\n",
        "        if creation_date and expiration_date:\n",
        "            registration_length = (expiration_date - creation_date).days\n",
        "            domain_age = (datetime.datetime.now() - creation_date).days\n",
        "        else:\n",
        "            registration_length = -1\n",
        "            domain_age = -1\n",
        "\n",
        "        logger.debug(f\"Registration Length: {registration_length} days, Domain Age: {domain_age} days\")\n",
        "\n",
        "        return domain_name, creation_date, expiration_date, registration_length, domain_age, None\n",
        "    except ValueError as e:\n",
        "        logger.error(f\"ValueError: {str(e)}\")\n",
        "        return None, None, None, -1, -1, f\"Value error: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Exception occurred: {str(e)}\")\n",
        "        return None, None, None, -1, -1, f\"Error fetching WHOIS info: {str(e)}\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    url = input(\"Enter the URL: \")\n",
        "    domain_name, creation_date, expiration_date, registration_length, domain_age, error_message = get_whois_info(url)\n",
        "\n",
        "    if domain_name:\n",
        "        print(f\"Domain: {domain_name}\")\n",
        "        print(f\"Creation Date: {creation_date}\")\n",
        "        print(f\"Expiration Date: {expiration_date}\")\n",
        "        print(f\"Registration Length (days): {registration_length}\")\n",
        "        print(f\"Domain Age (days): {domain_age}\")\n",
        "    else:\n",
        "        print(f\"Error: {error_message}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB3V6MmTXHII",
        "outputId": "d93527f6-737c-4753-961c-f7226cc2b366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the URL: www.youtube.com\n",
            "Domain: ['YOUTUBE.COM', 'youtube.com']\n",
            "Creation Date: 2005-02-15 05:13:12\n",
            "Expiration Date: 2025-02-15 05:13:12\n",
            "Registration Length (days): 7305\n",
            "Domain Age (days): 7151\n"
          ]
        }
      ]
    }
  ]
}